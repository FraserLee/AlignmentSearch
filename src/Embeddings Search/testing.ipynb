{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```source: this key separates the various keys found in the table in Sources. Here's the set of sources with their corresponding value name:```\n",
    "```\n",
    "'https://aipulse.org'\n",
    "'ebook'\n",
    "'https://qualiacomputing.com'\n",
    "'alignment forum'\n",
    "'lesswrong'\n",
    "'manual'\n",
    "'arxiv'\n",
    "'https://deepmindsafetyresearch.medium.com/'\n",
    "'waitbutwhy.com'\n",
    "'GitHub'\n",
    "'https://aiimpacts.org'\n",
    "'arbital.com'\n",
    "'carado.moe'\n",
    "'nonarxiv_papers'\n",
    "'https://vkrakovna.wordpress.com'\n",
    "'https://jsteinhardt.wordpress.com'\n",
    "'audio-transcripts'\n",
    "'https://intelligence.org'\n",
    "'youtube'\n",
    "'reports'\n",
    "'https://aisafety.camp'\n",
    "'curriculum'\n",
    "'https://www.yudkowsky.net'\n",
    "'distill'\n",
    "```\n",
    "\n",
    "```...and this is how the arxiv papers look like:```\n",
    "\n",
    "```\n",
    "{\n",
    "    \"source\": \"arxiv\", # where the dataset comes from\n",
    "    \"source_type\": \"latex\", # the type of file the data was original in\n",
    "    \"converted_with\": \"pandoc\", # which tool we used to convert the data in .md format\n",
    "    \"paper_version\": paper_id,\n",
    "    \"title\": title,\n",
    "    \"authors\": [str(x) for x in authors], # list of authors\n",
    "    \"date_published\": date_published,\n",
    "    \"data_last_modified\": data_last_modified,\n",
    "    \"url\": url,\n",
    "    \"abstract\": abstract,\n",
    "    \"author_comment\": author_comment,\n",
    "    \"journal_ref\": journal_ref,\n",
    "    \"doi\": doi,\n",
    "    \"primary_category\": primary_category,\n",
    "    \"categories\": categories,\n",
    "    \"citation_level\": citation_level, # (0 = curated alignment papers, 1 = citation of curated papers, 2 = citation of citation, etc.)\n",
    "    \"alignment_text\": is_alignment_text, # 'pos' is maunally labeled as an alignment paper, 'unlabeled' if unlabeled\n",
    "    \"confidence_score\": confidence_scores, # this is a confidence score obtained by using the SPECTER model to classify papers to add to the dataset\n",
    "    \"main_tex_filename\": \"main.tex\", # the main latex file needed to convert the paper\n",
    "    \"text\": \"lots of text\", # this is where you will grab the text contents of each entry in the dataset (in .md format)\n",
    "    \"bibliography_bbl\": \"string of bbl\",\n",
    "    \"bibliography_bib\": \"string of bib\", # more common to have bib than bbl\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aipulse.org: title links link authors author text (tags)\n",
    "\n",
    "None: title url text\n",
    "\n",
    "ebook: title book_title authors text (publication_date)\n",
    "\n",
    "https://qualiacomputing.com: title link authors author text (tags)\n",
    "\n",
    "alignment forum: title url authors text (tags)\n",
    "\n",
    "lesswrong: title authors url text (tags score date_published)\n",
    "\n",
    "manual: title authors text (date_published)\n",
    "\n",
    "arxiv: title authors url text (citation_level alignment_text confidence_score date_published)\n",
    "\n",
    "https://deepmindsafetyresearch.medium.com/: title url text\n",
    "\n",
    "waitbutwhy.com: title authors text (date_published)\n",
    "\n",
    "GitHub: book_title authors author text\n",
    "\n",
    "https://aiimpacts.org: title link authors author text (tags)\n",
    "\n",
    "arbital.com: title authors url text (date_published)\n",
    "\n",
    "carado.moe: title authors text (date_published)\n",
    "\n",
    "nonarxiv_papers: title authors doi text (date_published)\n",
    "\n",
    "https://vkrakovna.wordpress.com: title link authors author text (tags)\n",
    "\n",
    "https://jsteinhardt.wordpress.com: title link authors author text (tags)\n",
    "\n",
    "audio-transcripts: title authors text (date_published)\n",
    "\n",
    "https://intelligence.org: title link authors author text (tags)\n",
    "\n",
    "youtube: title authors url text (date_published)\n",
    "\n",
    "reports: title authors doi text (date_published)\n",
    "\n",
    "https://aisafety.camp: title link authors author text (tags)\n",
    "\n",
    "curriculum: title authors text (date_published)\n",
    "\n",
    "https://www.yudkowsky.net: title link authors author text (tags)\n",
    "\n",
    "distill: title authors doi text (date_published)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import openai\n",
    "import concurrent.futures\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "import config\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_EMBEDDINGS = 1536\n",
    "\n",
    "project_path = Path(__file__).parent.parent.parent\n",
    "PATH_TO_DATA = project_path / \"data\" / \"alignment_texts.jsonl\" # Path to the dataset .jsonl file.\n",
    "PATH_TO_EMBEDDINGS = project_path / \"src\" / \"Embeddings Search\" / \"data\" / \"embeddings.npy\" # Path to the saved embeddings (.npy) file.\n",
    "PATH_TO_DATASET = project_path / \"src\" / \"Embeddings Search\" / \"data\" / \"dataset.pkl\" # Path to the saved dataset (.pkl) file.\n",
    "\n",
    "COMPLETIONS_MODEL = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "MAX_LEN_PROMPT = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://stackoverflow.com/a/31505798/16185542\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"?!\", \"?\")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    if sentences == []:\n",
    "        sentences = [text.strip()]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingDataException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    def __init__(self, block_maxsize: int = 800, block_minsize: int = 500):\n",
    "        self.block_maxsize = block_maxsize\n",
    "        self.block_minsize = block_minsize\n",
    "        self.blocks = []\n",
    "        self.current_block = []\n",
    "        self.current_block_len = 0\n",
    "\n",
    "\n",
    "    def add_sentence_to_blocks(self, sentence):\n",
    "        sentence_len = len(sentence)\n",
    "        sentence_fits_in_current_block = self.current_block_len + sentence_len <= self.block_maxsize\n",
    "        current_block_is_big_enough = self.current_block_len >= self.block_minsize\n",
    "        sentence_fits_in_standalone_block = sentence_len <= self.block_maxsize\n",
    "\n",
    "        if sentence_fits_in_current_block:\n",
    "            self.current_block.append(sentence)\n",
    "            self.current_block_len += sentence_len + 1 # +1 for the space\n",
    "            return\n",
    "        \n",
    "        if current_block_is_big_enough and sentence_fits_in_standalone_block:\n",
    "            self.blocks.append(\" \".join(self.current_block))\n",
    "            self.current_block = [sentence]\n",
    "            self.current_block_len = sentence_len + 1 # +1 for the space\n",
    "            return\n",
    "        \n",
    "        #special cases:TODO refactor\n",
    "        #case 1: current_block_len < block_minsize and current_block_len + sentence_len > block_maxsize\n",
    "        #case 2: current_block_len > block_minsize but sentence_len > block_maxsize\n",
    "        shorter_sentence = sentence[self.block_maxsize - self.current_block_len]\n",
    "        self.current_block.append(shorter_sentence)\n",
    "        self.blocks.append(\" \".join(self.current_block))\n",
    "        self.current_block = []\n",
    "        self.current_block_len = 0      \n",
    "        \n",
    "\n",
    "    def add_paragraph_to_blocks(self, paragraph):\n",
    "        paragraph_len = len(paragraph)\n",
    "        if self.current_block_len + paragraph_len > self.block_maxsize:\n",
    "            sentences = split_into_sentences(paragraph)\n",
    "            for sentence in sentences:\n",
    "                self.add_sentence_to_blocks(sentence)\n",
    "            return\n",
    "        \n",
    "        if self.block_minsize <= self.current_block_len + paragraph_len <= self.block_maxsize:\n",
    "            self.current_block.append(paragraph)\n",
    "            self.blocks.append(\"\\n\\n\".join(self.current_block))\n",
    "            self.current_block = []\n",
    "            self.current_block_len = 0\n",
    "            return\n",
    "        \n",
    "        if self.current_block_len + paragraph_len < self.block_minsize:\n",
    "            self.current_block.append(paragraph)\n",
    "            self.current_block_len += paragraph_len + 2 # +2 for the \\n\\n\n",
    "            return\n",
    "        \n",
    "    def add_text_to_blocks(self, text):\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        for paragraph in paragraphs:\n",
    "            self.add_paragraph_to_blocks(paragraph)\n",
    "        if self.current_block != []:\n",
    "            self.blocks.append(\"\\n\\n\".join(self.current_block))\n",
    "\n",
    "\n",
    "    def split(self, text: str, signature: str) -> List[str]:\n",
    "        \"\"\"Split text into multiple blocks and add signature to each block.\"\"\"\n",
    "        # signature has the format : \"link, title, author\"\n",
    "        self.add_text_to_blocks(text)\n",
    "        blocks = self.blocks\n",
    "        self.blocks = []\n",
    "        self.current_block = []\n",
    "        self.current_block_len = 0\n",
    "        if blocks == []:\n",
    "            raise MissingDataException(\"No blocks were created\")\n",
    "        return [f\"{block}\\n - {signature}\" for block in blocks]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_count_dict = {\n",
    "    \"Entry has no source.\": 0,\n",
    "    \"Entry has no title.\": 0,\n",
    "    \"Entry has no text.\": 0,\n",
    "    \"Entry has no URL.\": 0,\n",
    "    \"Entry has wrong citation level.\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,\n",
    "            path: str,  # Path to the dataset .jsonl file.\n",
    "            sources: List[str] = None,  # List of sources to include. If None, include all sources.\n",
    "            rate_limit_per_minute: int = 3_500,  # Rate limit for the OpenAI API.\n",
    "            block_min_max_size: Tuple[int, int] = None,  # Tuple of (min_block_size, max_block_size), used for the text splitter. If None, use default values.\n",
    "            fraction_of_articles_to_use: float = 1.0,  # Fraction of articles to use. If 1.0, use all articles.\n",
    "        ):\n",
    "        self.path = path\n",
    "        self.sources = sources\n",
    "        self.rate_limit_per_minute = rate_limit_per_minute\n",
    "        self.delay_in_seconds = 60.0 / self.rate_limit_per_minute\n",
    "        self.fraction_of_articles_to_use = fraction_of_articles_to_use\n",
    "        \n",
    "        # Set up text splitter\n",
    "        if block_min_max_size is None: self.block_min_max_size = (400, 600)\n",
    "        else: self.block_min_max_size = block_min_max_size\n",
    "        self.text_splitter = TextSplitter(block_maxsize=self.block_min_max_size[1], block_minsize=self.block_min_max_size[0])\n",
    "        \n",
    "        self.data: List[Tuple[str]] = []  # List of tuples, each containing the title of an article, its URL, and text. E.g.: [('title', 'url', 'text'), ...]\n",
    "        self.embed_split: List[str] = []  # List of strings, each being a few paragraphs from a single article (not exceeding 1000 words).\n",
    "        \n",
    "        self.num_articles: Dict[str, int] = {}  # Number of articles per source. E.g.: {'source1': 10, 'source2': 20, 'total': 30}\n",
    "        if sources is None:\n",
    "            self.num_articles['total'] = 0\n",
    "        else:\n",
    "            for source in sources: \n",
    "                self.num_articles[source] = 0\n",
    "            self.num_articles['total'] = 0\n",
    "        \n",
    "        self.total_char_count = 0\n",
    "        self.total_word_count = 0\n",
    "        self.total_sentence_count = 0\n",
    "        self.total_block_count = 0\n",
    "        \n",
    "    def get_info_tmp(self):\n",
    "        self.sources_so_far = []\n",
    "        self.info_types: Dict[str, List[str]] = {}\n",
    "        with jsonlines.open(self.path, \"r\") as reader:\n",
    "            for entry in reader:\n",
    "                if 'source' not in entry: entry['source'] = 'None'\n",
    "                \n",
    "                if entry['source'] not in self.sources_so_far:\n",
    "                    self.sources_so_far.append(entry['source'])\n",
    "                    self.info_types[entry['source']] = entry.keys()\n",
    "                \n",
    "                if 'tags' in entry:\n",
    "                    print(entry['tags'])\n",
    "                    \n",
    "                \"\"\"\n",
    "                {\n",
    "                'text', \n",
    "                'title', 'book_title', # If there is both, take title, otherwise take book_title\n",
    "                'author', 'authors', # If there is both, take author, otherwise take authors, otherwise take author\n",
    "                'citation_level', # must be 0 or 1\n",
    "                'date_published', 'published', # take first 10 chars of date_published, if it exists; else take first 16 chars of published, if it exists\n",
    "                'doi', 'link', 'links', 'url', # if link, take link; elif url, take url; elif doi, take doi\n",
    "                'tags'\n",
    "                }\n",
    "                \"\"\"\n",
    "    \n",
    "    def get_alignment_texts(self):\n",
    "        with jsonlines.open(self.path, \"r\") as reader:\n",
    "            for entry in reader:\n",
    "                random_number = random.random()\n",
    "                if random_number > self.fraction_of_articles_to_use: continue\n",
    "                try:\n",
    "                    if 'source' not in entry: raise MissingDataException(\"Entry has no source.\")\n",
    "                    \n",
    "                    if self.sources is None:\n",
    "                        if entry['source'] not in self.num_articles: self.num_articles[entry['source']] = 1\n",
    "                        else: self.num_articles[entry['source']] += 1\n",
    "                        self.num_articles['total'] += 1\n",
    "                    else:\n",
    "                        if entry['source'] in self.sources:\n",
    "                            self.num_articles[entry['source']] += 1\n",
    "                            self.num_articles['total'] += 1\n",
    "                        else: continue\n",
    "                    \n",
    "                    text=title=author=citation_level=date_published=url=tags=None\n",
    "                    \n",
    "                    # Get text\n",
    "                    if 'text' in entry and entry['text']: text = entry['text']\n",
    "                    else: raise MissingDataException(f\"Entry has no text.\")\n",
    "                    \n",
    "                    # Get title\n",
    "                    if 'title' in entry and 'book_title' in entry and entry['title']: title = entry['title']\n",
    "                    elif 'book_title' in entry and 'title' not in entry and entry['book_title']: title = entry['book_title']\n",
    "                    elif 'title' in entry and entry['title']: title = entry['title']\n",
    "                    else: title = None\n",
    "                    if title[-1] == '\\n': title = title[:-1]\n",
    "                        \n",
    "                    # Get author\n",
    "                    if 'author' in entry and 'authors' in entry and entry['author']: author = entry['author']\n",
    "                    elif 'authors' in entry and entry['authors']: author = entry['authors']\n",
    "                    elif 'author' in entry and entry['author']: author = entry['author']\n",
    "                    else: author = None\n",
    "                        \n",
    "                    # Get citation level\n",
    "                    if 'citation_level' in entry:\n",
    "                        if entry['citation_level'] != 0: raise MissingDataException(f\"Entry has citation_level {entry['citation_level']}.\")\n",
    "                    \n",
    "                    # Get date published\n",
    "                    if 'date_published' in entry and entry['date_published'] and len(entry['date_published']) >= 10: date_published = entry['date_published'][:10]\n",
    "                    elif 'published' in entry and entry['published'] and len(entry['published']) >= 16: date_published = entry['published'][:16]\n",
    "                    else: date_published = None\n",
    "                        \n",
    "                    # Get URL\n",
    "                    if 'link' in entry and entry['link']: url = entry['link']\n",
    "                    elif 'url' in entry and entry['url']: url = entry['url']\n",
    "                    elif 'doi' in entry and entry['doi']: url = entry['doi']\n",
    "                    else: url = None\n",
    "                        \n",
    "                    # Get tags\n",
    "                    if 'tags' in entry and entry['tags']:\n",
    "                        if type(entry['tags']) == list: tags = ', '.join([val['term'] for val in entry['tags']])\n",
    "                        elif type(entry['tags']) == str: tags = entry['tags']\n",
    "                        else: tags = None\n",
    "                    \n",
    "                    signature = \"\"\n",
    "                    if title: signature += f\"Title: {title}, \"\n",
    "                    if author: signature += f\"Author: {author}, \"\n",
    "                    if date_published: signature += f\"Date published: {date_published}, \"\n",
    "                    if url: signature += f\"URL: {url}, \"\n",
    "                    # if tags: signature += f\"Tags: {tags}, \"\n",
    "                    if signature: signature = signature[:-2]\n",
    "\n",
    "                    self.data.append((title, author, date_published, url, tags, text))\n",
    "                    \n",
    "                    blocks = self.text_splitter.split(text, signature)\n",
    "                    self.embed_split.extend(blocks)\n",
    "                    \n",
    "                    self.total_char_count += len(entry['text'])\n",
    "                    self.total_word_count += len(entry['text'].split())\n",
    "                    self.total_sentence_count += len(split_into_sentences(entry['text']))\n",
    "                    self.total_block_count += len(blocks)\n",
    "                \n",
    "                except MissingDataException as e:\n",
    "                    if str(e) not in error_count_dict:\n",
    "                        error_count_dict[str(e)] = 0\n",
    "                    error_count_dict[str(e)] += 1\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        # Get an embedding for each text, with retries if necessary\n",
    "        @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "        def get_embedding(text: str, delay_in_seconds: float = 0) -> np.ndarray:\n",
    "            time.sleep(delay_in_seconds)\n",
    "            result = openai.Embedding.create(model=EMBEDDING_MODEL, input=text)\n",
    "            return result[\"data\"][0][\"embedding\"]\n",
    "        \n",
    "        embeddings = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(get_embedding, text) for text in self.embed_split]\n",
    "            num_completed = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                embeddings.append(future.result())\n",
    "                num_completed += 1\n",
    "                if num_completed % 50 == 0:\n",
    "                    print(f\"Completed {num_completed}/{len(self.embed_split)}\")\n",
    "        self.embeddings = np.vstack(embeddings)\n",
    "    \n",
    "    def save_embeddings(self, path: str):\n",
    "        np.save(path, self.embeddings)\n",
    "        \n",
    "    def load_embeddings(self, path: str):\n",
    "        self.embeddings = np.load(path)\n",
    "        \n",
    "    def save_class(self, path: str):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(path=PATH_TO_DATA, sources=None, rate_limit_per_minute=3500, block_min_max_size = [1200, 1500], fraction_of_articles_to_use=1/1000)\n",
    "dataset.get_alignment_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A study in Science on memory conformity\n",
      "Author: dvasya\n",
      "Date published: 2011-07-15\n",
      "URL: https://www.lesswrong.com/posts/fQE3cP9NjS5sLm8wr/a-study-in-science-on-memory-conformity\n",
      "Tags: Conformity Bias\n",
      "Text: I believe this may be a good addition to the cognitive bias literature:\n",
      "\n",
      "\n",
      "# > Following the Crowd: Brain Substrates of Long-Term Memory Conformity\n",
      "\n",
      "\n",
      " - Micah Edelson1,*, \n",
      "\n",
      " - Tali Sharot2, \n",
      "\n",
      " - Raymond J. Dolan2, \n",
      "\n",
      " - Yadin Dudai1\n",
      "\n",
      "1Department of Neurobiology, Weizmann Institute of Science, Israel.\n",
      "\n",
      "\n",
      " - 2Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, London, UK.\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Human memory is strikingly susceptible to social influences, yet we know little about the underlying mechanisms. We examined how socially induced memory errors are generated in the brain by studying the memory of individuals exposed to recollections of others. Participants exhibited a strong tendency to conform to erroneous recollections of the group, producing both long-lasting and temporary errors, even when their initial memory was strong and accurate. Functional brain imaging revealed that social influence modified the neuronal representation of memory. Specifically, a particular brain signature of enhanced amygdala activity and enhanced amygdala-hippocampus connectivity predicted long-lasting but not temporary memory alterations. Our findings reveal how social manipulation can alter memory and extend the known functions of the amygdala to encompass socially mediated memory distortions.\n",
      "\n",
      "\n",
      "http://​​www.sciencemag.org/​​content/​​333/​​6038/​​108.full\n",
      "\n",
      "http://​​ifile.it/​​v76wsi5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_num = 0\n",
    "print(f\"Title: {dataset.data[article_num][0]}\")\n",
    "print(f\"Author: {dataset.data[article_num][1]}\")\n",
    "print(f\"Date published: {dataset.data[article_num][2]}\")\n",
    "print(f\"URL: {dataset.data[article_num][3]}\")\n",
    "print(f\"Tags: {dataset.data[article_num][4]}\")\n",
    "print(f\"Text: {dataset.data[article_num][5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "{'total': 46, 'lesswrong': 37, 'alignment forum': 2, 'arxiv': 6, 'https://jsteinhardt.wordpress.com': 1}\n",
      "223697\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.data))\n",
    "print(dataset.num_articles)\n",
    "print(dataset.total_char_count)\n",
    "print(len(dataset.embed_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 50/179\n",
      "Completed 100/179\n",
      "Completed 150/179\n"
     ]
    }
   ],
   "source": [
    "dataset.get_embeddings()\n",
    "dataset.save_embeddings(PATH_TO_EMBEDDINGS)\n",
    "dataset.save_class(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe this may be a good addition to the cognitive bias literature:\n",
      "\n",
      "\n",
      "# > Following the Crowd: Brain Substrates of Long-Term Memory Conformity\n",
      "\n",
      "\n",
      " - Micah Edelson1,*, \n",
      "\n",
      " - Tali Sharot2, \n",
      "\n",
      " - Raymond J. Dolan2, \n",
      "\n",
      " - Yadin Dudai1\n",
      "\n",
      "1Department of Neurobiology, Weizmann Institute of Science, Israel.\n",
      "\n",
      "\n",
      " - 2Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, London, UK.\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Human memory is strikingly susceptible to social influences, yet we know little about the underlying mechanisms. We examined how socially induced memory errors are generated in the brain by studying the memory of individuals exposed to recollections of others. Participants exhibited a strong tendency to conform to erroneous recollections of the group, producing both long-lasting and temporary errors, even when their initial memory was strong and accurate. Functional brain imaging revealed that social influence modified the neuronal representation of memory. Specifically, a particular brain signature of enhanced amygdala activity and enhanced amygdala-hippocampus connectivity predicted long-lasting but not temporary memory alterations. Our findings reveal how social manipulation can alter memory and extend the known functions of the amygdala to encompass socially mediated memory distortions.\n",
      " - Title: A study in Science on memory conformity, Author: dvasya, Date published: 2011-07-15, URL: https://www.lesswrong.com/posts/fQE3cP9NjS5sLm8wr/a-study-in-science-on-memory-conformity\n",
      "\n",
      "\n",
      "http://​​www.sciencemag.org/​​content/​​333/​​6038/​​108.full\n",
      "\n",
      "http://​​ifile.it/​​v76wsi5\n",
      "\n",
      " - Title: A study in Science on memory conformity, Author: dvasya, Date published: 2011-07-15, URL: https://www.lesswrong.com/posts/fQE3cP9NjS5sLm8wr/a-study-in-science-on-memory-conformity\n",
      "\n",
      "## Design sketches\n",
      "\n",
      "1. Notes on differences between these images and what we’re planning on implementing:\n",
      "\n",
      "\n",
      "\n",
      "2. Meetup widget in sidebar, listing the 5 nearest meetups occurring in the next 2 weeks, linking to:(something like right-click & \"Open Image in new tab\" will help here)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. New headers:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " - Title: Official Less Wrong Redesign: Nearly there, Author: matt, Date published: 2011-05-24, URL: https://www.lesswrong.com/posts/FXMQWrkNKcZNagZRS/official-less-wrong-redesign-nearly-there\n",
      "\n",
      "Link post\n",
      "\n",
      " - Title: Instrumental Rationality and Overriding Defaults, Author: lifelonglearner, Date published: 2017-01-19, URL: https://www.lesswrong.com/posts/AT4NjNxav9TGoBSZu/instrumental-rationality-and-overriding-defaults\n",
      "\n",
      "The word \"optimizer\" can be used in at least two different ways. First, a system can be an \"optimizer\" in the sense that it is solving a computational optimization problem. A computer running a linear program solver, a SAT-solver, or gradient descent, would be an example of a system that is an \"optimizer\" in this sense. That is, it runs an *optimization algorithm*. Let \"optimizer_1\" denote this concept. Second, a system can be an \"optimizer\" in the sense that it optimizes its environment. A human is an optimizer in this sense, because we robustly take actions that push our environment in a certain direction. A reinforcement learning agent can also be thought of as an optimizer in this sense, but confined to whatever environment it is run in. This is the sense in which \"optimizer\" is used in posts such as this. Let \"optimizer_2\" denote this concept. These two concepts are distinct. Say that you somehow hook up a linear program solver to a reinforcement learning environment. Unless you do the \"hooking up\" in a particularly creative way there is no reason to assume that the output of the linear program solver would push the environment in a particular direction. Hence a linear program solver is an optimizer_1, but not an optimizer_2. On the other hand, a simple tabular RL agent *would* eventually come to systematically push the environment in a particular direction, and is hence an optimizer_2.\n",
      " - Title: Two senses of \"optimizer\", Author: Joar Skalse, Date published: 2019-08-21, URL: https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer\n",
      "\n",
      "However, such a system does not run any internal optimization algorithm, and is therefore not an optimizer_1. This means that a system can be an optimizer_1 while not being an optimizer_2, and vice versa. There are some arguments related to AI safety that seem to conflate these two concepts. In *Superintelligence *(pg 153), on the topic of Tool AI, Nick Bostrom writes that: > A second place where trouble could arise is in the course of the software’s operation. If the methods that the software uses to search for a solution are sufficiently sophisticated, they may include provisions for managing the search process itself in an intelligent manner. In this case, the machine running the software may begin to seem less like a mere tool and more like an agent. Thus, the software may start by developing a plan for how to go about its search for a solution. The plan may specify which areas to explore first and with what methods, what data to gather, and how to make best use of available computational resources. In searching for a plan that satisfies the software’s internal criterion (such as yielding a sufficiently high probability of finding a solution satisfying the user-specified criterion within the allotted time), the software may stumble on an unorthodox idea. For instance, it might generate a plan that begins with the acquisition of additional computational resources and the elimination of potential interrupters (such as human beings).\n",
      " - Title: Two senses of \"optimizer\", Author: Joar Skalse, Date published: 2019-08-21, URL: https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer\n",
      "\n",
      "To me, this argument seems to make an unexplained jump from optimizer_1 to optimizer_2. It begins with the observation that a powerful Tool AI would be likely to optimize its internal computation in various ways, and that this optimization process could be quite powerful. In other words, a powerful Tool AI would be a strong optimizer_1. It then concludes that the system might start pursuing convergent instrumental goals – in other words, that it would be an optimizer_2. The jump between the two is not explained. The implicit assumption seems to be that an optimizer_1 could turn into an optimizer_2 unexpectedly if it becomes sufficiently powerful. It is not at all clear to me that this is the case – I have not seen any good argument to support this, nor can I think of any myself. The fact that a system is internally running an optimization algorithm does not imply that the system is selecting its output in such a way that this output optimizes the environment of the system. The excerpt from *Superintelligence* is just one example of an argument that seems to slide between optimizer_1 and optimizer_2. For example, some parts of *Dreams of Friendliness *seem to be doing so, or at least it’s not always clear which of the two is being talked about. I’m sure there are more examples as well. Be mindful of this distinction when reasoning about AI. I propose that \"consequentialist\" (or perhaps \"goal-directed\") is used to mean what I have called \"optimizer_2\".\n",
      " - Title: Two senses of \"optimizer\", Author: Joar Skalse, Date published: 2019-08-21, URL: https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer\n",
      "\n",
      "I don’t think there is a need for a special word to denote what I have called \"optimizer_1\" (at least not once the distinction between optimizer_1 and optimizer_2 has been pointed out). Note: It is possible to raise a sort of embedded agency-like objection against the distinction between optimizer_1 and optimizer_2. One might argue that: > There is no sharp boundary between the inside and the outside of a computer. An \"optimizer_1\" is just an optimizer whose optimization target is defined in terms of the state of the computer it is installed on, whereas an \"optimizer_2\" is an optimizer whose optimization target is defined in terms of something outside the computer. Hence there is no categorical difference between an optimizer_1 and an optimizer_2. I don’t think that this argument works.  - A computer that is able to very quickly solve very large linear programs. - A computer that solves linear programs, and tries to prevent people from turning it off as it is doing so, etc. System 1 is an optimizer_1 that solves linear programs, whereas system 2 is an optimizer_2 that is optimizing the state of the computer that it is installed on. These two things are different. (Moreover, the difference isn’t just that system 2 is \"more powerful\" than system 1 – system 1 might even be a better linear program solver than system 2.\n",
      " - Title: Two senses of \"optimizer\", Author: Joar Skalse, Date published: 2019-08-21, URL: https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer\n",
      "\n",
      ") Acknowledgements: We were aware of the difference between \"optimizer_1\" and \"optimizer_2″ while working on the mesa-optimization paper, and I’m not sure who first pointed it out.\n",
      "\n",
      "We were also probably not the first people to realise this.\n",
      " - Title: Two senses of \"optimizer\", Author: Joar Skalse, Date published: 2019-08-21, URL: https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer\n",
      "\n",
      "In the first weekend of this year, the Future of Life institute hosted a landmark conference in Puerto Rico: \"The Future of AI: Opportunities and Challenges\". The conference was unusual in that it was not made public until it was over, and the discussions were under Chatham House rules. The slides from the conference are now available. The list of attenders includes a great many famous names as well as lots of names familiar to those of us on Less Wrong: Elon Musk, Sam Harris, Margaret Boden, Thomas Dietterich, all three DeepMind founders, and many more.\n",
      "\n",
      "This is shaping up to be another extraordinary year for AI risk concerns going mainstream!\n",
      "\n",
      " - Title: Slides online from \"The Future of AI: Opportunities and Challenges\", Author: Paul Crowley, Date published: 2015-01-16, URL: https://www.lesswrong.com/posts/TJSRiqBJxbcwEo7AR/slides-online-from-the-future-of-ai-opportunities-and\n",
      "\n",
      "Erik DeBenedictis on supercomputing \n",
      " Erik DeBenedictis works for Sandia’s Advanced Device Technologies department. He has been a member of the International Technology Roadmap for Semiconductors since 2005.\n",
      "DeBenedictis has received Ph.D. in computer science from Caltech. As a grad student and post-doc, he worked on the hardware that turned into the first hypercube multiprocessor computer. Later dubbed the “Cosmic Cube,” it ran for more than a decade after he left the university and was copied over and over. It’s considered the ancestor of most of today’s supercomputers.\n",
      "In the 1980s, then working for Bell Labs in Holmdel, N.J., DeBenedictis was part of a consortium competing for the first Gordon Bell award. The team got the second place award, the first place going to Sandia. During the 1990s, he ran NetAlive, Inc., a company developing information management software for desktops and wireless systems. Starting in 2002, DeBenedictis was one of the project leads on the Red Storm supercomputer. \n",
      "The opinions expressed by Erik below are his own and not those of Sandia or the US Department of Energy. This document has been released by Sandia as SAND Number 2014-2679P. Luke Muehlhauser: Some of your work involves reversible computing, which I previously discussed with Mike Frank. Mike’s view seemed to be that there were promising signs that reversible computing would be possible eventually, but progress is not moving quickly due to lack of funding and interested researchers.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Is that your view as well? And based on my interview with him, do you seem to have a substantially different impression that Mike does about anything he and I discussed? Erik DeBenedictis: I agree with Mike, but his discussion of minimum energy in computing due to irreversibility is just part of a larger topic of minimum energy in computing that starts with “Moore’s Law Ending”. For any reader who has not read Mike Frank’s interview, I’d like to give a quick summary of the relevant points. Mike was interviewed about reversible logic, which is sometimes called reversible computing. If you were a brilliant engineer and could figure out how to make a computer logic gate like AND or OR that dissipated kT joules per logic operation (the meaning of kT is in the next paragraph), you would discover that there is an additional heat production on the order of kT due to the interaction between information and thermodynamics. If you were determined to make even lower power computer gates anyway, you would have to use reversible logic principles. You could use a different universal gate set that would include a new gate such as the TOFFOLI or FREDKIN gate. You could also use regular gates (e. g. AND, OR, NOT) and a “retractile cascade” clocking scheme that reverses the computation after you capture the answer. For reference on kT: k = 1.38 x 10-23 Joules/Kelvin is Boltzmann’s constant and T is the absolute temperature with T = 300 Kelvin at room temperature.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "kT is about 4 zeptojoules = 4 x 10-21 Joules. Comparing this number to today’s computers is imprecise because dissipation in today’s computers is primarily attributable to the interconnect wire, which varies in length. An AND or OR gate in a modern computer may dissipate a million times this value. A great many respected scientists believe that reversible computing is feasible, but challenging. If their views are correct, computation should be possible at “arbitrarily low energy levels” and all theories proposing unavoidable, general limits are incorrect. There are a handful of contrary theories proposing minimum energy dissipation levels for computation. Several key ones are Landauer’s Limit of “on the order of kT” per logic operation1, a thermal limit of 40-100 kT (depending on your definition of reliable), and the concept in the popular press today that “Moore’s Law is Ending” and the minimum energy per computation is whatever is in the rightmost column of the International Technology Roadmap for Semiconductors (ITRS). That value is about 50,000 kT with typical lengths of interconnect wire. Scientific situations with multiple competing theories can be settled by a scientific experiment. For example, there is a researcher in New York that has a superconducting circuit running in the sub-kT range and looks like it could demonstrate a logic circuit in another couple “spins” of his chip.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Demonstrating and rigorously measuring a sub-kT circuit would invalidate all current theories claiming unavoidable limits. Whether anybody will fund such an experiment should depend on whether anybody cares about the result, and I’d like to present two society-level questions that the experiment would resolve: The computer industry started its upward trend during WWII, growing industry revenue and computer throughput in a fairly clean exponential lasting 70 years. The revenue from semiconductors and downstream industries is around $7 trillion per year right now. If there is a lower energy limit to computing, the shift in growth rate will cause a glitch in the world’s economy. My argument is that proving or disproving theories of computing limits could be accomplished for a very small fraction of $7 trillion per year. The second has to do with profoundly important computational problems, such as the simulation of the global environment to assess climate change issues. Existing climate models running on petaflops supercomputers give varying projections for the future climate, with these projections diverging from observations over the last decade. Regardless of politics, the remedy would be a more sophisticated climate model running on a bigger supercomputer. We don’t know how much bigger, but a zettaflops or more has been mentioned in this context.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "If any of the minimum energy dissipation theories are correct, the energy dissipation of the required supercomputer could turn out to be too large and climate modeling may be infeasible; if the theory that computing is possible at “arbitrarily low levels” is true, accurate climate modeling will just require a highly-advanced computer.\n",
      "\n",
      "I’ve tried to expand on Mike’s point: Research on reversible computing could shed light on the future of the economy and the planet’s climate, but I do not know of a single person funded for reversible computing research.\n",
      "\n",
      "Furthermore, a conclusive demonstration of reversible computing would show that there is plenty of room for improving computer efficiency and hence performance.\n",
      "\n",
      "If “Moore’s Law is Ending” means an end to improving computer efficiency, validating reversible computing would show this to be a matter of choice not technology.\n",
      "\n",
      "Luke: From your perspective, what are the major currently-foreseeable barriers that Moore’s law might crash into before hitting the Landauer limit? (Here, I’m thinking more about the economically important “computations per dollar” formulations of Moore’s law rather than the “serial speed” formulation, which hit a wall in 2004.)\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Erik: There is huge upside, but not necessarily for every application. The “computations per dollar” link in the question focused on the use of computers as a platform for strong Artificial Intelligence (AI), so I will comment specifically on that application: I wouldn’t be surprised to see AI accelerated by technology specifically for learning, like neural networks with specialized devices for the equivalent of synapses.\n",
      "Let’s consider (a) Moore’s Law 1965 to say 2020 and (b) Beyond Moore’s Law 2020+.\n",
      "From 1965 to 2020, the strategy was to shrink line width. That strategy will be good for 1012 or so increase in computations per dollar.\n",
      "I see the following classes of advances beyond 2020 that will each give maybe 10-100x efficiency increase each: More efficient implementation of the von Neumann architecture. More parallelism, with a commensurate increase in the difficulty of programming. Software improvements for more efficient execution (e. g. new computer languages and compilers to run general code on Graphics Processing Units). Better algorithms that solve a given problem with fewer computations. Accelerators, such as CPU+GPU today, extendable to CPU+GPU+various new accelerator types. Even at constant energy per gate operation, continued scaling in 2D and better use of the third dimension for reducing communications energy. Optical interconnect has upside, but optics is often oversold.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Nanodevices with behavior different from a transistor that allow some computer functions to be done more efficiently. Examples: Memristors, analog components. Improved gate technology through adiabatic methods, sub-threshold or low-threshold operation, or probabilistic computing. Eventually, reversible computing (note below on this one). Alternative models of computation (i.e. neuromorphic) that do not use gates as normally defined. If the ten items in the list above yield an average of 1½ orders of magnitude increase in computations per dollar each, you have more upside than the entire run of Moore’s Law. If a couple of the items in the list don’t pan out, you could achieve excellent results by concentrating on other paths. So I do not see a general technology crash anytime soon. However, certain specific applications may be dependent on a just a subset of the list above (climate modeling was mentioned) and could be vulnerable to a limit. Reversible computing plus continued reduction in manufacturing cost per device could extend upside potential tremendously. However, the necessary technology investment will be greater in the future for a less clear purpose. The message of Moore’s Law was very concise: industry and government invest in line width shrinkage and get a large payoff. In the future, many technology investments will be needed whose purposes have less clear messages.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Bottom line: In general, the path ahead is expensive but will yield a large increase in computations per dollar. Specific application classes could see limits, but they will have to be analyzed specifically. Luke: What’s your opinion on whether, in the next 15 years, the dark silicon problem will threaten Moore’s Law (computations per dollar)? Erik: I believe the dark silicon problem will negatively impact computations per dollar. The problem and the underlying energy efficiency problem are going to get worse at least until the cost of increased energy is greater than the cost of refining a solution and bringing it to production. That will happen eventually, but I believe the problem will persist longer than may be expected due momentum against change. However, you admit Moore’s Law has ended when you admit that there is a dark silicon problem. The underlying cause of dark silicon is that technology scales device dimensions faster than it reduces power. This causes power per unit chip area to increase, which contradicts the key statement in Gordon Moore’s 1965 paper that defined Moore’s Law: “In fact, shrinking dimensions on an integrated structure makes it possible to operate the structure at higher speed for the same power per unit area”. The mismatched scaling rates create a problem for computations per dollar. Today, the cost of buying a computer is approximately equal to the cost of supplying it with power over its lifetime.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Unless power efficiency can be increased, improvements to computer logic will not benefit the user because the amount of computation they use will be limited by the power bill. The mismatched scaling rates can be accommodated (but not solved) by turning off transistors (dark silicon), packing microprocessors with low energy-density functions like memory (a good idea, to a point), and specialization (described in your interview under dark silicon problem). The scaling rates could be brought together by more power-efficient transistors, such as the Tunnel Field Effect Transistor (TFET). However, this transistor type will only last a few generations. See here. Theory says energy per computation can be made “arbitrarily small,” but R&D to exploit these issues will be expensive and disruptive. The leading approaches I am aware of are: Adiabatic. A fundamentally different approach to logic gate circuits. Example: Mike Frank’s 2LAL. Certain low-voltage logic classes: For example, see CMOS LP in arXiv 1302.0244 (which is not same as ITRS CMOS LP). Reversible computing, the topic of Mike Frank’s interview. The approaches above are disruptive, which I believe limits their popularity today. The approaches use different circuits from CMOS, which would require new design tools. New design tools would be costly to develop and would require retraining of the engineers that use them.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Children learn the words “and” and “or” when they are about one year old, with these words becoming the basis of AND and OR in the universal logic basis of computers. To exploit some technologies that save computer power, you have to think in terms of a different logic basis like TOFFOLI, CNOT, and NOT. Some of the ideas above would require people to give up concepts that they learned as infants and have not had reason to question before. Luke: What do you mean by “you admit Moore’s Law has ended when you admit that there is a dark silicon problem”? The computations-per-dollar Moore’s Law has held up at least through early 2011 (I haven’t checked the data after that), but we’ve known about the dark silicon problem since 2010 or earlier. Erik: Moore’s Law has had multiple meanings over time, and is also part of a larger activity. There was a very interesting study by Nordhaus that revealed the peak computation speed of large computers experienced an inflection point around WW II and has been on an upwards exponential ever since. Eyeballing figure 2 of his paper, I’d say the exponential trend started in 1935. Gordon Moore published a paper in 1965 with the title “Cramming more Components onto Integrated Circuits” that includes a graph of components per chip versus year.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "As I mentioned for a previous question, the text of the paper includes the sentence, “In fact, shrinking dimensions on an integrated structure makes it possible to operate the structure at higher speed for the same power per unit area”. The graph and previous sentence seem to me to be a subjective description of an underlying scaling rule that was formalized by Dennard in 1974 and is called Dennard scaling. I have sketched below Moore’s graph of components as a function of year with Nordhaus’ speed as a function of year on the same axes (a reader should be able to obtain the original documents from the links above, which are more compelling than my sketch). This exercise reveals two things: (1) the one-year doubling period in Moore’s paper was too fast, and is now known to be about 18 months, and (2) that Moore’s Law is a subtrend of the growth in computers documented by Nordhaus. A really interesting question is whether Moore was applying somebody else’s law or whether the two laws were actually part of a larger concept that was not understood at the time. I conclude the latter. Intel did not invent the microprocessor until six years after Moore’s article. I have also talked to people (not Moore) who tell me Gordon Moore was thinking about general electrical circuits and was not foreseeing the emergence of the microprocessor. Let me try to apply Moore’s Law as defined by his paper.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "I recall building a computer system in 1981 with an 8086 (very similar to the 8088 in the original IBM PC). I’d heard it was highly complex and dissipated a lot of heat, so I put my finger on it to experience the heat. I recall surprise that it didn’t seem warmer than anything else. I have thought about the heat from microprocessors in the last year, 33 years later. Since Moore’s Law says power per unit area is the same and chips are nearly the same size at 1 cm2, I should be able to put my finger on a chip and not feel any heat. The reality is that there is a new structure sitting on top of today’s microprocessors that reminds me of Darth Vader’s head and is called a “heat sink”. The heat sink is to remove 50-200 watts of heat generated by the chip. I believe I’ve just made a case that any microprocessor with a heat sink violates Moore’s Law. What’s going on? Moore’s Law is being given additional meaning over and above what Moore was thinking. Many people believe Moore’s Law is only about dimensional scaling, a conclusion supported by the title of his article and the main graph. Moore’s Law has also been associated with computations per dollar, but that law had been around for 30 years before Moore’s paper. I found the interview with Hadi Esmaeilzadeh on Dark Silicon to be on track, yet he uses another interpretation of Moore’s Law – one where Moore’s Law continues, but Dennard scaling ended in the mid-2000s.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Yet, I quoted the phrase from Moore’s paper that disclosed the scaling rule that later became known as Dennard scaling. At a higher level, I believe Moore’s Law has turned into a marketing phrase that is being redefined as needed by the semiconductor industry so it remains true. So why are computations per dollar rising? For many years, the vendor objective was to make processors that ran word processors and web browsers faster. This trend culminated in the early 2000s with processors like the Pentium IV with a 4 GHz clock and dissipating 200W+. Customers rebelled and industry shifted to multicore. With an n-core microprocessor, the results of running the benchmark on one core could be multiplied by n. This is an example of progress (raising computations per dollar) by item 2 in my response to a previous question (more parallelism, subject to difficulty in programming). Even now, most software does not exploit the multiple cores. Luke: You write that “Customers rebelled and industry shifted to multicore”. I typically hear a different story about the 2002-2006 era, one that didn’t have much to do with customer rebellion, but instead the realization by industry that the quickest way to keep up the Moorean trend — to which consumers and manufacturers had become accustomed — was to jump to multicore. That’s the story I see in e.g. The Future of Computing Performance by National Academies Press (official summary here).\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Moreover, the power scaling challenge to Moore’s Law was anticipated many years in advance by the industry, for example in the ITRS reports.\n",
      "\n",
      "Can you clarify what you mean by “customers rebelled”?\n",
      "\n",
      "Erik: What happens if you take projections of the future to be true and then the projections change? You eventually end up with multiple “truths” about the same thing in the historical record. I accept that the stories you hear are true, but there is another truth based on different projections.\n",
      "Let us mathematically invert the ITRS roadmap to see how projections of today’s (2014) microprocessor clock rate evolved as industry addressed power scaling and shifted to multicore. I have gone back to earlier editions of the ITRS and accessed edition reports for 2003, 2005, and 2007. In table 4 of the executive summary of each edition, they have a projection of “on chip local clock,” which means microprocessor clock rate. I accessed Pricewatch to get the 2014 clock rate.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "On chip local clock\n",
      "In year 2013\n",
      "In year 2014\n",
      "In year 2015\n",
      "\n",
      "\n",
      "Projection in 2003 ITRS\n",
      "22.9 GHzTable 4c\n",
      "Only odd years reported in this edition\n",
      "33.4 GHzTable 4d\n",
      "\n",
      "\n",
      "Projection in 2005 ITRS\n",
      "\n",
      "28.4 GHzTable 4d\n",
      "\n",
      "\n",
      "\n",
      "Projection in 2007 ITRS\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "7.91 GHzTable 4c\n",
      "\n",
      "\n",
      "\n",
      "2014 reality\n",
      "\n",
      "4.0 GHz\n",
      "Pricewatch.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The most conspicuous issue is that the 2003 and 2005 editions overstated clock rate by about 7x. ITRS accommodated to multicore in 2007 with a new scaling model that we see in retrospect overstates reality by only 2x. Footnote 1 in the 2007 ITRS describes the change. The footnote ends with the following sentence: “This is to reflect recent on-chip frequency slowing trends and anticipated speed-power design tradeoffs to manage a maximum 200 watts/chip affordable power management tradeoff.”\n",
      "If you believe ITRS is “industry,” industry had been telling customers to expect the benefits of Moore’s Law through rising clock rate. In my view, customers took the lead in saying power per chip should be less than 200 watts even if it meant a more difficult to use parallel programming model. Several years after the multicore became popular, industry changed its projection so customers were to expect the benefits of progress through rising computations per dollar rather than speed. This, of course, led to the rise of battery operated smart phones and tablets with power limits much lower than 200 watts.\n",
      "By the way, I have not heard the phrase “Moorean trend” before. It seems to capture the idea of progress in computing without being tied to particular technical property. Why don’t you trademark it; it gets zero Google hits.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Luke: Are you willing to make some forecasts about the next ~15 years in computing? I’d be curious to hear your point estimate, or even better your 70% confidence interval, for any of the following: FLOPS per US dollar in top-end supercomputing in 2030.\n",
      "Average kT per active logic gate in top-end supercomputing in 2030.\n",
      "Some particular measure of progress on reversible computing, in 2030?\n",
      "World’s total FLOPS capacity in 2030. (See here.) Or really, anything specific about computing you’d like to forecast for 2030. Erik: The FLOPS per dollar question will be most interesting, so I’ll leave it for last. kT/logic op: I see a plateau around 10,000 kT, and will discuss what might come beyond the plateau in the next paragraph. . My guess of 10,000 kT includes interconnect wire, which is significant because today 75-90% of energy is attributable to interconnect wire. Today, we see around 50,000 kT. A reduction in supply voltage to . 3v should be good for 10x improvement, but there are other issues. This estimate should be valid in 10 years, but the question asked about 15 years. I would not be surprised that we see a new approach in the interval 2025-2030 (mentioned below). It will be difficult to predict specifically, but the five-year interval is short and the improvement rate seems to be insensitive to details. So, say there is a 5x additional improvement by 2030. Cumulative by 2030: 2,000 kT/logic op, including interconnect wire. However, this will be really disappointing.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "People will expect 10 doublings due to Moore’s Law in the 15-year interval, for an expected improvement of 1024x; I’m predicting 25x. Reversible computing: I think reversible computing (as strictly defined) will be demonstrated in a few years and principally impact society’s thought processes. The demonstration would be computation at less than 1 kT/logic op, where theory says those levels are unachievable unless reversible computing principles are used. I do not expect reversible computing to be widely used by 2030. The projection of 2,000 kT/logic op in 2030 represents a balance of manufacturing costs and energy costs. By 2030, reversible computing could be employed in some applications where power is very expensive, such as spacecraft or implantable medical devices. However, a demonstration of reversible computing could have an important impact on societal thinking. Popular thinking sees some ideas as unlimited for planning purposes and endows those ideas with attention and investment. This applied to California real estate prices (until 2008) and Moore’s Law (until a few years ago). Claims that “Moore’s Law is Ending” are moving computation into a second class of ideas that popular thinking sees as limited, like the future growth potential of railroads. A reversible computing demonstration would move computing back to the first category and thus make more attention and capital available. However, reversible computing is part of a continuum.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "I see a good possibility that adiabatic methods could become the new method mentioned above for the 2020-2025 time range. World’s Total FLOPS capacity, 2030. I looked over the document by Naik you cited. I don’t feel qualified to judge his result. However, I will stand by my ratio of 50,000 kT to 2,000 kT = 25. So my answer is to multiply Naik’s result by 25. I do not imagine that the cumulative power consumption of computers will rise substantially, particularly with Green initiatives. FLOPS per dollar: This answer will be all over the place. Let’s break down by application class: (A) Some applications are CPU-bound, meaning their performance will track changes in kT per logic op. I have given my guess of 25x improvement (which is a lot less then the 1024x that Moore’s Law would have delivered). (B) Other applications are memory bound, meaning their performance will track (a) memory subsystem performance, where advances partially overlap with advances due to Moore’s Law and (b) architecture changes that can reduce the amount of data movement. It is a lot easier to make a computer for (A) than (B); for a given cost, a computer will outperform type A applications by an order of magnitude or more on FLOPS compared to type B. A top-end supercomputer supports both A and B, but the balance between A and B may be the profound question of the era. The balance has been heavily weighted in favor of A (through reliance on LINPACK as the benchmark).\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "However, we do not currently have a particularly aggressive Exascale program in the US. Instead, we have a lot of discussion about the memory subsystem’s low energy efficiency. You can make a fairly compelling case that progress in top-end supercomputing will be held up until the computers can become better balanced. (For reference the TOP 2 supercomputer is ORNL Titan with 17.5 Petaflops Linpack for $97 million; a ratio of 181 MFLOPS/$. The TOP 1 supercomputer does not seem to be a good cost reference. ) If architecture stays fixed until 2030, I’ll guess 25x improvement. That would be 4.5 GFLOPS/$. Memory subsystems are made out of the same transistor technology as logic, perhaps plus a growing fraction of optics. If transistors become more effective by 25x, this could benefit both FLOPS and the memory subsystem. Use of 3D may boost performance (due to shorter wires), but this will be offset by efficiency loss due to difficulty exploiting greater parallelism. Call the latter factors a wash. Architecture is the wildcard. There are architectures known that are vastly more efficient than the von Neumann machine, such as systolic arrays, Processor-In-Memory (PIM), Field Programmable Gate Arrays (FPGAs) and even GPUs. These architectures get a performance boost by organizing themselves to put calculations closer to where data is stored, requiring less time and energy to complete a task. Unfortunately, these architectures succeed at the expense of generality.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "If a vendor boosts performance through too much specialization, they run the risk of being disqualified as an example of a “top-end supercomputer”. The Holy Grail would be a software approach that would make general software run on some specialized hardware (like a compiler that would run general C code on a GPU – at the full performance of the GPU). However, I will predict that architecture improvements will contribute an additional 4x by 2030, for a cumulative improvement factor of 100x. That will be 18 GFLOPS/$. This is still 10x short of the 1024x expected for 15 years. However, I think Artificial General Intelligence (AGI) may fare well due to specialization. Synaptic activity is the dominant function that enables living creatures to think, but it is quite different from the floating point in a supercomputer. A synapse performs local, slow, computations based on analog stimuli and analog learned behavior. In contrast, the floating point in a supercomputer operates blazingly fast on data fetched from a distant memory and computes an answer with 64-bit precision. Speed reduces energy efficiency, and the supercomputer doesn’t even learn. Since a von Neumann computer is Turing complete, it will be capable of executing an AGI coded in software. However, the efficiency may be low. Executing an AGI could be optimized by new or specialized technology and advance faster than the rate of Moore’s Law, like Bitcoin mining.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "I am going to project that an AGI demonstration at scale will require a non-conventional, but not unimaginable computer.\n",
      "\n",
      "The computer could be specialized CMOS, like a GPU with special data types and data layout.\n",
      "\n",
      "Alternatively, the computer could employ new physical devices, such as a neuromorphic architecture with a non-transistor device (e.\n",
      "\n",
      "g. memristor).\n",
      "\n",
      "All said, AGI might see 1000x or more improvement.\n",
      "\n",
      "In other words, AGI enthusiasts might be able to plan on 181 GFLOPS/$ by 2030.\n",
      "\n",
      "However, they would be classed as AI machines rather than top-end supercomputers.\n",
      "\n",
      "Luke: Thanks, Erik!\n",
      "Note added in review: Landauer proposed a lower limit on “on the order of kT” only for “irreversible” computations. As far as I know, the phrase “Landauer’s Limit” was created later by other people. In my experience, the phrase “Landauer’s Limit” if often applied as a general limit.The post Erik DeBenedictis on supercomputing appeared first on Machine Intelligence Research Institute.\n",
      " - Title: Erik DeBenedictis on supercomputing, Author: Luke Muehlhauser, Date published: Thu, 03 Apr 2014, URL: https://intelligence.org/2014/04/03/erik-debenedictis/\n",
      "\n",
      "Speaker  00:03 Hello and Welcome to Session 146 in the AC T comm reading group. Tonight we'll be discussing the second part of categories article, likelihood of discontinuous progress around the development of artificial general intelligence. Catch a grace is probably the author of this piece, and the lead scientist at the AI impacts projects. We're discussing the second half of the article wherein categories describes her summary of the arguments for this discontinuous progress, and why she is not convinced that the likelihood is particularly large. And as before, I've tried to add some kind of answers both to the presentation of the arguments and to categorises counter arguments, mostly from the book super intelligence. But before I get into that, I would like to once again say thank you to catch a grace. Getting a high quality, critical thoughts and scepticism about these arguments is really important. They're my counter arguments this time are a bit more disjointed than last time, because my true objection for why I believe there is a large likelihood of discontinuous progress was for the section intelligence explosion that we discussed last time. So for this reason, in the other. In the other arguments, I'm kind of trying to push it towards this talk about Bostrom six, two superpowers that we had last week. I also tried to include some other counter arguments where I see them, but I'm less convinced about them, but I wanted to add them anyway.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "So the first section is called train versus test. And, and it also includes searching actually, so it should maybe be search versus train versus test. And test in this case is deploying and executing. So it might take, just like with a metaphor for martial arts, you spent much more time training that you do fighting. And this is the basic idea, compute can be used for three things, both searching the space of all possible software programmes that can be written and see which ones are AGI eyes. And once you have something that is perhaps an AGI, then you can try to train it. And once you have trained it, then you can try to run it. And there is a hierarchy here in that the first takes much longer than the second, and the second takes much longer than the third. This is just some kind of discontinuity in impact, in that if you are at a if you have computer processes, processes that are good enough for step two, then they can probably run the AGI very, very fast. And that might create some kind of more impact than you'd otherwise think. catchphrases counter argument for this is that the AGI will still while it's an improvement before it will be used, is it still something that replaces slightly worse software. And that means the discontinuity we'll be seeing is not a discontinuity on a high level capability, but only a small step ahead of what was previously the state of the art.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "One of the things that I noticed here was the the the argument the kind of steel men first talked about when we discover AGI and catch a grace here is talking about the gradual step towards AGI. And it's something that will, at first be only a tiny, tiny, while continues bit better than what we have before. So So I guess this hints at one of the key philosophical differences there is, will a is there a missing key to general intelligence that we haven't discovered? Yes, I think most scientists would say yes, that there is still some mysteries locked in the brain. But of course, it is possible even when there is that AGI will be gradual, but it's also possible that it will be much more abrupt. Now for the the actual counter-arguments. A week one that I just realised was that of course, if we have training time, that is very long on the order of years, then this year, we will have some kind of we'll have better hardware to do most law and this kind of thing. And that might be another reason why the AGI would be potentially faster. That's not really my true objection. My true reaction is the only example we have of General Lynch origins are humans, training a human takes roughly 18 years give or take, and 18 years is actually requires quite a lot of compute to go through 18 years.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "So if we imagine that we have two, two systems, one with an IQ of 100, and the other with an IQ of 101, so an extremely continuous, a tiny, tiny bit of difference, but then the, if we can train them in 18 years, we can probably just as easily execute them for 18 years. So if you imagine 18 people thinking for a year, that's the same as one person thinking for 18 years, if you have the people with an IQ of 101 101, then what would be the difference? My intuition is that since it goes, since the the slightly better, algorithms will be around for so much longer time, there could be a strong discontinuous difference between the two. Another issue is that this train time of 18 years, is something that evolution has forced on humans for because we need to be able to procreate and, and this kind of thing. So humans need to learn quickly. But actually, it's not, it's not obvious that 18 years at all, is the optimal in any, for any particular reason. It might be that to reach a higher level of AGI, you need an even greater difference between training time and execution time. But it's of course also possible it can go the other way, it might be that training time is not appreciably longer than testing time, we don't know basically, because we do not have AGI yet. Now, if we include searching for computer programmes, then we imagine some kind of three step model where we start by searching for programmes, and then the ones we have found, we train them.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "And once we've trained, we test them, then this is a loop that requires a lot more, a lot more computing power. This, the difference between train and test is something that we that is brought up from time to time, but searching programmes. That's not something people talk a lot about. Because searching for programmes is in general something that computer scientists only do if they have no other choice, because it is normally extraordinary, computationally expensive. In people off more often talk about the search for programmes in the context of what like the universal prior and what programmes we can find there. And a lot of people's think that that is like some of the most unsafe AIS, but let's assume that we can't build AGI in any other ways. So we will find it by searching. So we're in the far future now. Now, how much more space does it take to find computer programmes compared to running them? Well, probably almost certainly enough that just if you need to search, then train a particular computer programme needs to be basically trivial. So the factors that I intuitively, intuitively think about here is that searching compared to training, then searching takes a billion billions of times more time. And then of course, training and test might also be a building of a factor. So this means that the factors here are not like the difference between 118 people thinking for one year, but like billions of people thinking for billions of years.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "And of course here that would magnify the differences to a truly discontinuous extent. The next argument is about starting Hi. Where can you create is as an example, using the Wright brothers. Wright Flyer three, I believe this picture is I that was actually in the beginning, I made a no categories promise that there will be a separate treatment of the case where inventions go from zero to one. And I said that that wasn't in the first part, but actually it is in the second part here. So this argument, is that the first item in a trend, that's a discontinuity in the opposite sense that the amount of examples go from zero to one. And so this is claimed by category to not be very interesting. The first, there's a metric on how many AGI systems do we have? And that's not really interesting. I would actually kind of think for the radical point of view and all these kind of things. It's actually really, really interesting, but that's it not interesting, according to Bostrom, six superpowers. Now the problem here is that the development of AGI that could cause some metrics to go from zero to some high number according to social impact. And when we think about Boston's superpowers, then we actually don't care about going from the level siru. To human level, that's unlikely to have great impact. What's going to happen, great impact is going from human level to above. Now, how often does it happen on the outside view, that new technologies have this kind of high social impact?\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "The example that categories uses is aeroplane size, which Wright Flyer was 40 feet. So that's kind of large, and there was never a time where the best aircraft were atomic size, they started at 40 feet and only grow larger by that. But aeroplane size, that's not really a very socially impactful metric. And, again, here, as before, we are playing this kind of this game of reference class tennis where categories are throwing a lot of examples are saying, maybe the development of AGI would be like the development. Well, not really like the development of aircraft, but our planes size. Our plane size is a really, really odd metric. I've never heard about that before. And so I'm wondering if this could be some kind of strawman. I think the same argument might be make with how long time the first right fly could fly, right? There was never a time where there was aircraft. But But aircrafts were not able to fly for one minute, they did start out being able to fly, you know, appreciably length and flying for one minute, that's not that bad, really. Now, my, my true objection about this is that, in the example with Wright Flyer, what they what the brothers right, actually did, that was truly groundbreaking, was to make a lot of groundwork, they built models in wind tunnels, and did a huge amount of work on actually building this these aircrafts.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "And that was actually where their true their true contribution was, in figuring out how to build them, and not so much in actually flying them. That was more like, Okay, that was maybe also it's kind of a contribution, but the big thing were all the preliminary work they did, but they never went public with that. And the public probably would not have care about this, the public cares about inventions. At the time, when it is above the alternative, they don't care about that you have made a better wind tunnel and figuring out how to do that in a good way. They become public in at the level where they are above the alternative. There are some cases where people don't make them public at this particular case, the military have a number of things where they have kept secrets, there are companies that are kept secrets, they probably keep them secret in the hope of a maybe discontinuous, decisive strategic advantage, or something else. So my point here is that it's not a coincidence that we often see inventions made public at precisely the first level where they are above the the alternative. That's because the incentives to make things public, make that the right choice. Of course, this is a really imperfect model I haven't thought so much about it's possible to get quite a few to think of quite a few exceptions to this trend. Now catchphrases, counter arguments go in a somewhat different direction.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "She claims that looking back, new technologies usually don't use these kind of discontinuities. The examples you she's using are very different from the ones I used in the in the previous presentation. We're talking about the agricultural revolution, the scientific revolution, the Industrial Revolution, and catcher Grace are thinking about some very different reference classes. She's claiming here, I'm quoting at length, it is hard to imagine AGI suddenly processing some new trade to a degree that will revolutionise the economy without this producing discontinuous change in previously measured things like ability to turn resources into financial profit. And I've come up with what I think is a reasonable counter example to this. If we imagine an AGI gradually improving. First it's capable of doing intellectual work according to an IQ of 80 and then It improves. So it's capable of doing intellectual work comparable to an IQ of 120. That would not give humanity as such any. Any new skills, we can't do anything that we couldn't do before, because there are people with an ad with a higher IQ. But this would, of course, still revolutionise the economy. Categories also says there's no trend where important technologies start high in general. And I probably agree with this. I don't really know how you would go about determining this in a robust way. But I think she's probably right. So I'm not gonna argue against that.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "Now, there are also the arguments for standing Hi, that we're using the inside view. And this, of course, is a picture from Bostrom spoke super intelligence showing the kinetics of an intelligence explosion. And so what are the reasons we should expect AGI looking to exceed this kind of base rate that we saw on the outside view, the base rate probability of standing high? Well, argument one is that it's a very novel technology. So we're more likely to see progress on metrics that have seen no progress before. And since I don't really care about these, most of these metrics, I care about six metrics, which are buzzwords, six superpowers, and of course, there have been dramatic progress on this. So I think I reject this argument out of the box, then there is the argument that full AGI would be unusually impactful. And that implies that minimum AGI would also be impactful. And here, if I can just go back to past Trump's graph here, where there's a takeoff starting here, where the AGI begins to contribute to its own development, and then it ends up with some kind of strong super intelligence here. And that means that the minimum functional AGI here is the first AGI that's capable of contributing to its own development that makes it impactful by definition on the superpower called intelligence amplification.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "So what we back basically will see is that anything below this level is something that the the project will work on, but not really publicised or used for anything in particular. So it is basically for all intents and purposes, not AGI but an AGI being built. At least, that's one way of looking at AGI and that's probably not how catch a grace looks at AGI. But unfortunately, she doesn't really comment on this kinetics of an intelligence explosion from mushrooms, books, so I can't really see what I'm arguing against here. The third argument on the inside view is that it's possible that AGI is for some reason not not amenable to coming in functional, but low quality. And the reason why I believe this is that in this the time until take off, while the AGI is being built, we will see the AGI that it might be functional, but as it has too low quality to actually do anything, then most likely the researchers will not will not release it or do much other things with this project, except just improving it until it gets to the level where it's able to meaningfully contribute to its own development. Categories this counter argument against again go in a somewhat different direction. The original argument that if fully developed AGI is unusual, impactful, this suggests the minimal functional AGI also will be unusually impactful. And catchy grace gives us as an example, humans as an general intelligence that is imperfect but functional. And that is indeed true.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "And that is of course precisely what what is going on in classrooms. Description of this takeoff, where humans and the minimal AGI collaborate to improve the AGI. So, one way to see this in short ways that if if it's not high if the AGI is not impactful, then it's not really AGI then it has a start. Categories also use this as a positive example of showing how it's possible to have functional AGI but not have an intelligence explosion. And that is usually explained by hardware limitation to rise. But actually I think hardware is only rather small Part of it, the true reason why humans haven't done and intelligence explosion is that we can't do brain surgery on ourselves. But with software, you can actually edit software in a much easier way. The next document is on Alpha zero, which is claimed to be some kind of discontinuity, at least within games, and perhaps also other metrics. So that means that technologists like Alpha zero probably also have some kind of greater pro probability of producing discontinuities, categories, countless, that the matrix that officer was also on, were not really interesting. And they she's not really examining other AI progress, particularly closely forward discontinuities. She doesn't believe that this is particularly relevant. I actually feel that it might be relevant. Go and chess are examples of matrix where people actually had put quite a lot of thought into how to build chess, chess games and go and go algorithms.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "So I think calling it reasonably optimised before alphazero can be justified, then whether it is worthwhile to look at other AI progress? Well, I actually think I don't, this is not my true objection. But I think she's going a bit too quickly over this. The article is called likelihood of discontinuous AI progress. And then saying we don't care about a how many discontinuities there are in AI progress. That seems like an obviously relevant thing that is excluded here. But that's truly my objection. So in general, what I feel Elsa serum is proof of is that it is apparently possible in AI to have these kinds of discontinuities, probably because AI is not truly optimised the same way that humans optimise, trying to predict stock prices or things like that. There's just not enough people who are working in DeepMind, or comparable places. Then the next argument is about uneven skills, where the argument is that if we want to replace a human doing skill, A, B, C, D, and E, then we need to have some kind of ability on all of them. And if once the human level AI is above, on all, in particular skill, see here, then on the other skills like D, we might have something that is very strongly superhuman. I'm not quite sure about this argument to any particular degree, I feel that we're using the word general intelligence. And this general intelligence probably can be used reasonably interchangeable, not perfectly, but reasonably interchangeable for all these skills.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "So that's more like my intuition about how this work. But I don't have very strong arguments for this catchphrases, counter arguments, this is that this is actually something we see whenever one kind of process replace and other. And since categories do leave that discontinuities are rare in practice, then from that follows this dynamic doesn't really tend to produce great discontinuities. My answer, one of the weaker as I add to this is that the six superpowers that we're looking at, they are extremely socially impactful. So for this reason, we are actually also interested in in even minor discontinuities in the categories further argues that humans are not generally replaced in one go, but step by step. And the problem is that this is how it's been so far. And this is how it's been for machines and also for narrow AI. But AGI is truly different. On the inside view in the we are replacing the core of what makes humans able to perform a lot of these skills. So for this reason, there is a much greater reason to expect powerful AGI to replace basically all humans in one go. Then there is a lot of thresholds. The last two arguments are about thresholds, both payoff thresholds and human competition thresholds.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "I feel it might be better to take them at least a bit together in the argument is given the first if we assume we have some kind of linear progress in AGI then that might still cause some very discontinuous change in economic value and ability to obtain it the session To take advantage, we have even more examples here, AGI might be comparable to a boat, in that building better boats might not help you until you have a boat that's actually capable of reaching the next island. So if the universe is made up of Ireland's in this way, then there might be some thresholds that will cause this continuous change. And it is also possible that this threshold might not be in nature. As the example with the ions, it might be that going from not competitive to humans to slightly better than humans would yield a qualitatively different outcome. I feel here that this is actually from two different sections. And I feel they're mixed somewhat together in categories, this description. If we take samples from the pay, not from the human competition thresholds, then its economic value and ability to take over the world. And this seems like something that relates very strongly to competing with humans, we want to have economic value than the way and AGI has economic value is by being better than humans or comparable to humans, and taking over the world. While humans have the world right now. So it's taking over the world from humans. Catch a grace has yet another example of what AGI might be.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "And she claims that AGI might be comparable to the value of a projectile fired from a gun according to its speed. This is another rather strange example. So the assumption is, if the market correctly predicts what is the value according to the speed of the projectile, then the market will assign more resources to building guns that are worse on all other metrics, but have a higher speed and a more expensive so that the this extra resources will go to having a gun that reached precisely this threshold in speed. And in the equilibrium there's there'll be no discontinuity around the time when a gun with this particular speed hits the market. I feel this is probably true. But it hinges on that the market is correctly able to predict how will what will be the impact of artificial general intelligence. And I think this is unlikely there has been cases where it has been true, we have seen things like the invention of the light bulb, where there was an enormously long period where people tried to build light bulbs, and then they put some that were extremely expensive and only could only be used and then gradually was better and better. So arguably, the value created by the light bulb was introduced in a completely discontinuous way. There are many other examples of markets failing to predict technologies. One of the strongest example I found was in was actually not directly related to technology.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "But the Cuban Missile Crisis, which potentially would have had an enormous influence on on the market, where the market basically didn't react at all to the world coming to the brink of nuclear annihilation. I feel this is a I mean, David might disagree here, but there is a limit to what the market can predict. In particular, I feel that when a technology will be if there is ever a technology that will allow an ex to obtain decide strategic advantage. My strong intuition is that the market will not price it correctly in advance. Because basically what price would it be it would be infinitely high. Then there's a bit more about the human competition thresholds finally, and here I'm basically not summarising catchphrases article, but more going out with a with my own interpretation of the arguments. And that is the human competition threshold seem to be particularly important in six domains, intelligence amplification, hacking Bussum, six superpowers, basically, those are particularly relevant in how strongest the AI compared to humans in these activities. Categories answer is that human skills vary a lot. So it's actually not much of a threshold because there is a huge difference in how productive are humans and how skilled are they at social manipulation, etc. That might be true, but some of them are not done by People who score very Allah, I've tried to sort them here.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n",
      "So I feel like, for instance, intelligence amplification in a, in an intelligence explosion scenario, this is something that will not be done by most people, but be done by a modestly small team at DeepMind or something like that, which is a tiny, tiny percentage of the human variation.\n",
      "\n",
      "For hacking also, this is where the skip the top skill matter lot, and the lowest schools don't really matter.\n",
      "\n",
      "So we might imagine, to agents to AIS version 0.9 and 0.1.\n",
      "\n",
      "Again, we're the first hacker it's better than 99.9% of all humans.\n",
      "\n",
      "So that means that there are 7 million humans that are better than the than the as a hacker, and the different AI, which is slightly better, it is better than 100% of humans.\n",
      "\n",
      "And in this case, I would expect the first hacker to have basically no impact, because there isn't there's hardly 7 million people trying to stop hackers.\n",
      "\n",
      "Whereas if there is hacker there is that is better than the humans, then then the humans that are trying to stop the hacker, then that will would likely have an enormous impact because it might be able to make an attack that precisely zero of our current systems can can withstand.\n",
      "\n",
      "So for this reason, I believe that this kind of threshold might be really, really important.\n",
      "\n",
      "But that is all for today.\n",
      "\n",
      "Thank you and see you next week.\n",
      " - Title: 146. Discontinuous Progress 2-by AI Safety Reading Group-video_id j8GnMy-Bckk-da\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embed in dataset.embed_split:\n",
    "    print(embed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH_TO_DATASET, 'rb') as f:\n",
    "#     dataset2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentSearch:\n",
    "    def __init__(self,\n",
    "            dataset: Dataset,  # Dataset object containing the data.\n",
    "        ):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        result = openai.Embedding.create(model=EMBEDDING_MODEL, input=text)\n",
    "        return result[\"data\"][0][\"embedding\"]\n",
    "    \n",
    "    def get_top_k(self, query: str, k: int=10) -> List[str]:\n",
    "        # Receives a query (str) and returns the top k blocks that are most semantically similar to the query.\n",
    "        # Each tuple contains the title of an article, its URL, and text.\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        similarities = np.dot(self.dataset.embeddings, query_embedding)\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_k = [self.dataset.embed_split[i] for i in top_k_indices]\n",
    "        return top_k\n",
    "    \n",
    "    def construct_messages(self, question: str, blocks: List[str] = [\"None\"], mode: str = \"balanced\") -> str:\n",
    "        # Receives a question (str) and a list of blocks and returns a prompt (str) to be used for text generation.\n",
    "        context = \"\"\n",
    "        for i, block in enumerate(blocks):\n",
    "            context += f\"Context #{i+1}: {block}\\n\\n\"\n",
    "        context = context[:MAX_LEN_PROMPT * 3] + \"...\" if len(context) > MAX_LEN_PROMPT * 3 else context[:-2]\n",
    "        \n",
    "        if mode == \"balanced\":\n",
    "            assistant_prompt = \"You are a helpful assistant, and you help users by answering questions and providing information about AI Alignment and AI Safety. You are extremely knowledgeable, yet you know the limits of your own knowledge. Answer the user's questions as truthfully as possible using the provided context, and if the answer is not contained within it, say \\\"I don't know.\\\", or \\\"I'm not sure I know the answer to your question. However, I can try.\\\" followed by an attempt to answer as best you can. You can also ask the user questions to clarify their question.\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": assistant_prompt},\n",
    "                {\"role\": \"system\", \"content\": context},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "        elif mode == \"precise\":\n",
    "            raise NotImplementedError\n",
    "        elif mode == \"creative\":\n",
    "            raise NotImplementedError\n",
    "        elif mode == \"HyDE\":\n",
    "            assistant_prompt = \"You are a helpful assistant, and you help users by answering questions and providing information about AI Alignment and AI Safety, on which you are extremely knowledgeable. Answer the user's question even if you are not certain of the answer; it is supremely important that you do attempt to offer an answer related to the user's query.\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": assistant_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be one of 'balanced', 'precise', 'creative', or 'HyDE'.\")\n",
    "        return messages\n",
    "    \n",
    "    def answer_question(self, question: str, blocks: List[str]) -> str:\n",
    "        # Receives a question (str) and a list of blocks and returns an answer (str) to the question.\n",
    "        messages = self.construct_messages(question, blocks, mode=\"balanced\")\n",
    "        answer = openai.ChatCompletion.create(\n",
    "            model=COMPLETIONS_MODEL, \n",
    "            messages=messages\n",
    "        )\n",
    "        return answer[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    def search_and_answer(self, question: str, k: int=10, HyDE: bool=False) -> str:\n",
    "        # Receives a question (str) and returns an answer (str) to the question.\n",
    "        if HyDE:\n",
    "            messages = self.construct_messages(question, mode=\"HyDE\")\n",
    "            hyde_completion = openai.ChatCompletion.create(\n",
    "                model=COMPLETIONS_MODEL, \n",
    "                messages=messages\n",
    "            )\n",
    "            top_k = self.get_top_k(hyde_completion, k)\n",
    "        else:\n",
    "            top_k = self.get_top_k(question, k)\n",
    "        answer = self.answer_question(question, top_k)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Not necessarily. While it's possible that smarter AI could have a better understanding of ethics and morality, the relationship between intelligence and morality is not straightforward. Ethical behavior requires more than just intelligence; it requires values, empathy, and an understanding of complex human situations. Moreover, a smarter AI could use its intelligence to achieve goals that conflict with human values or lead to unintended consequences. Ensuring that AI is aligned with human values and ethical principles is a major focus of AI safety research.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678851324,\n",
      "  \"id\": \"chatcmpl-6uCD6zNQj230yYrUdYCHoUJ2Vabv1\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 98,\n",
      "    \"prompt_tokens\": 3256,\n",
      "    \"total_tokens\": 3354\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Not necessarily. While it's possible that smarter AI could have a better understanding of ethics and morality, the relationship between intelligence and morality is not straightforward. Ethical behavior requires more than just intelligence; it requires values, empathy, and an understanding of complex human situations. Moreover, a smarter AI could use its intelligence to achieve goals that conflict with human values or lead to unintended consequences. Ensuring that AI is aligned with human values and ethical principles is a major focus of AI safety research.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678851324,\n",
      "  \"id\": \"chatcmpl-6uCD6zNQj230yYrUdYCHoUJ2Vabv1\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 98,\n",
      "    \"prompt_tokens\": 3256,\n",
      "    \"total_tokens\": 3354\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "SA = AlignmentSearch(dataset=dataset)\n",
    "query = \"Is it true that smarter AI will also be more moral?\"\n",
    "# top_k = SA.get_top_k(query, 10)\n",
    "# print(top_k)\n",
    "answer = SA.search_and_answer(query, 10)#, HyDE=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s often not stated as an assumption; rather, inferences are made assuming that you have the background model that the AI is goal-directed. This makes it particularly hard to question the assumption, since you don’t realize that the assumption is even there. Another reason that this assumption is so easily accepted is that we have a long history of modeling rational agents as expected utility maximizers, and for good reason: there are many coherence arguments that say that, *given that you have preferences/​goals*, if you aren’t using probability theory and expected utility theory, then you can be taken advantage of. It’s easy to make the inference that a superintelligent agent must be rational, and therefore it must be an expected utility maximizer.\n",
      " - Title: More marbles and Sleeping Beauty, Author: Manfred, Date published: 2014-11-23, URL: https://www.lesswrong.com/posts/c7fAEStCafGh8TFdf/more-marbles-and-sleeping-beauty\n",
      "\n",
      " - A goal-conditioned policy with common sense could be operationalized as optimizing for the goal of \"following a human’s orders without doing anything that humans would reliably judge as crazy\".  - MIRI’s version of corrigibility seems like it stays within this framework. - You could think of the services in CAIS as optimizing for the *aggregate* reward they get over all time, rather than just the reward they get during the current episode. I do *not* want these versions of the scenarios, since they then make it tempting to once again say \"but if you get the goal even slightly wrong, then you’re in big trouble\".\n",
      " - Title: Sydney Rationality Dojo—November, Author: Cpt. Bl, Date published: 2019-01-02, URL: https://www.lesswrong.com/events/tMzE2FPbHroAJ54L4/sydney-rationality-dojo-november\n",
      "\n",
      "Because this assumption was so embedded in how I thought about the problem, I had trouble imagining how else to even consider the problem. I would guess this is true for at least some other people, so I want to summarize the counterargument, and list a few implications, in the hope that this makes the issue clearer. ### Why goal-directed behavior may not be required The main argument of this chapter is that it is not *required* that a superintelligent agent takes actions in pursuit of some goal. It is possible to write algorithms that select actions without doing a search over the actions and rating their consequences according to an explicitly specified simple function.\n",
      " - Title: LessWrong Study Hall will be password-protected, Author: tkadlubo, Date published: 2013-11-13, URL: https://www.lesswrong.com/posts/BCvED5FkJ9GBAwQ9M/lesswrong-study-hall-will-be-password-protected\n",
      "\n",
      "They might all read a few books worth of stuff, but they won’t all have read the same books. The information that they can be coordinated around is more like \"several blogposts\". If you’re trying to coordinate *nerds*, maybe those blogposts add up to one book because nerds like to read. If you want to coordinate 1,000 people… you realistically get one blogpost, or maybe one blogpost worth of jargon that’s hopefully self-explanatory enough to be useful. If you want to coordinate thousands of people... You have about five words. This has ramifications on how complicated a coordinated effort you can attempt. What if you *need* all that nuance *and* to coordinate thousands of people?\n",
      " - Title: Sydney Rationality Dojo—November, Author: Cpt. Bl, Date published: 2019-01-02, URL: https://www.lesswrong.com/events/tMzE2FPbHroAJ54L4/sydney-rationality-dojo-november\n",
      "\n",
      "As it turns out, nope, it’s the same exact game, just re-labeled.\n",
      "\n",
      "In the re-labeled marble game you still have two unknown variables (represented by flipping coins), and you still have a 1⁄2 chance of black and Tails, a 1⁄4 chance of black and Heads, and a 1⁄4 chance of white and Heads.\n",
      "\n",
      "And then to get the thirds, you ask the question \"If I get a black marble, what is the probability of the faces of the first coin?\" Now you update to P(Heads|black)=1/​3 and P(Tails|black)=2/​3.\n",
      "\n",
      "**II**\n",
      "\n",
      "Okay, enough analogies. What’s going on with these two positions in the Sleeping Beauty problem?\n",
      "\n",
      "**1:** ** 2:**\n",
      " - Title: More marbles and Sleeping Beauty, Author: Manfred, Date published: 2014-11-23, URL: https://www.lesswrong.com/posts/c7fAEStCafGh8TFdf/more-marbles-and-sleeping-beauty\n",
      "\n",
      "Because this assumption was so embedded in how I thought about the problem, I had trouble imagining how else to even consider the problem. I would guess this is true for at least some other people, so I want to summarize the counterargument, and list a few implications, in the hope that this makes the issue clearer. ### Why goal-directed behavior may not be required The main argument of this chapter is that it is not *required* that a superintelligent agent takes actions in pursuit of some goal. It is possible to write algorithms that select actions without doing a search over the actions and rating their consequences according to an explicitly specified simple function.\n",
      " - Title: Southern California FAI Workshop, Author: Scott Garrabrant, Date published: 2014-04-20, URL: https://www.lesswrong.com/posts/nqgY7mNy8JMN99SHA/southern-california-fai-workshop\n",
      "\n",
      "Once we let go of the idea of optimizing for a single goal and it becomes possible to think about other ways in which we could build AI systems, there are more insights about how we could build an AI system that does what we intend instead of what we say.\n",
      "\n",
      "(In my case it was reversed—I heard a lot of good insights that don’t fit in the framework of goal-directed optimization, and this eventually led me to let go of the assumption of goal-directed optimization.\n",
      "\n",
      ") We’ll explore some of these in the next chapter.\n",
      " - Title: [Link] Truth-telling is aggression in zero-sum frames (Jessica Taylor), Author: ioannes, Date published: 2019-09-11, URL: https://www.lesswrong.com/posts/T39zJNzXEeh9MR47h/link-truth-telling-is-aggression-in-zero-sum-frames-jessica\n",
      "\n",
      "### Implications At a high level, I think that the main implication of this view is that we should be considering other models for future AI systems besides optimizing over the long term for a single goal or for a particular utility or reward function. - G*oal-conditioned policy with common sense:* In this setting, humans can set goals for the AI system simply by asking it in natural language to do something, and the AI system sets out to do it. However, the AI also has \"common sense\", where it interprets our commands pragmatically and not literally: it’s not going to prevent us from setting a new goal (which would stop it from achieving its current goal), because common sense tells it that we don’t want it to do that.\n",
      " - Title: [Link] Truth-telling is aggression in zero-sum frames (Jessica Taylor), Author: ioannes, Date published: 2019-09-11, URL: https://www.lesswrong.com/posts/T39zJNzXEeh9MR47h/link-truth-telling-is-aggression-in-zero-sum-frames-jessica\n",
      "\n",
      " - A goal-conditioned policy with common sense could be operationalized as optimizing for the goal of \"following a human’s orders without doing anything that humans would reliably judge as crazy\".  - MIRI’s version of corrigibility seems like it stays within this framework. - You could think of the services in CAIS as optimizing for the *aggregate* reward they get over all time, rather than just the reward they get during the current episode. I do *not* want these versions of the scenarios, since they then make it tempting to once again say \"but if you get the goal even slightly wrong, then you’re in big trouble\".\n",
      " - Title: Southern California FAI Workshop, Author: Scott Garrabrant, Date published: 2014-04-20, URL: https://www.lesswrong.com/posts/nqgY7mNy8JMN99SHA/southern-california-fai-workshop\n",
      "\n",
      "One way to think about this is to consider an AI system that infers and follows human *norms*, which are probably much easier to infer than human values (most humans seem to infer norms very accurately).  - *Corrigible AI:* I’ll defer to Paul Christiano’s explanation of corrigibility. - *Comprehensive AI Services (CAIS):* Maybe we could create lots of AI services that interact with each other to solve hard problems. Each individual service could be bounded and episodic, which immediately means that it is no longer optimizing over the long term (though it could still be goal-directed).  \n",
      " - Title: Decentralization How—Los Angeles LW/​SSC Meetup #74 (Wednesday, September 12th), Author: T3t, Date published: 2018-09-12, URL: https://www.lesswrong.com/events/fbhEipkzZPwfFYPD3/decentralization-how-los-angeles-lw-ssc-meetup-74-wednesday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in top_k:\n",
    "    print(k)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source               Truth      Empirical  Difference\n",
      "total                41614      14         41600     \n",
      "lesswrong            28479      11         28468     \n",
      "alignment forum      2138       1          2137      \n",
      "arxiv                8007       2          8005      \n",
      "\n",
      "                     Truth      Empirical  Difference\n",
      "Word Count           53550146   23344      53526802  \n",
      "Character Count      351767163  143048     351624115 \n"
     ]
    }
   ],
   "source": [
    "num_articles_truth = {\n",
    "    'https://aipulse.org': 23,\n",
    "    'ebook': 23,\n",
    "    'https://qualiacomputing.com': 278,\n",
    "    'alignment forum': 2138,\n",
    "    'lesswrong': 28252 + 227,\n",
    "    'manual': \"?\",\n",
    "    'arxiv': 707 + 1679 + 1000 + 4621,\n",
    "    'https://deepmindsafetyresearch.medium.com/': 10,\n",
    "    'waitbutwhy.com': 2,\n",
    "    'GitHub': \"?\",\n",
    "    'https://aiimpacts.org': 227,\n",
    "    'arbital.com': 223,\n",
    "    'carado.moe': 59,\n",
    "    'nonarxiv_papers': \"?\",\n",
    "    'https://vkrakovna.wordpress.com': 43,\n",
    "    'https://jsteinhardt.wordpress.com': 39,\n",
    "    'audio-transcripts': 25 + 12,\n",
    "    'https://intelligence.org': 479,\n",
    "    'youtube': 457,\n",
    "    'reports': \"?\",\n",
    "    'https://aisafety.camp': 8,\n",
    "    'curriculum': \"?\",\n",
    "    'https://www.yudkowsky.net': 23,\n",
    "    'distill': 49,\n",
    "    'total': 2138+28252+707+1679+1000+4621+23+227+23+8+59+111+10+17+7+479+39+278+43+2+23+420+323+49+457+25+12+223+227+132    \n",
    "}\n",
    "word_count_truth = 53_550_146\n",
    "char_count_truth = 351_767_163\n",
    "\n",
    "# Print table. First row has Truth and Empirical findings.\n",
    "print(f\"{'Source':<20} {'Truth':<10} {'Empirical':<10} {'Difference':<10}\")\n",
    "for source in dataset.num_articles:\n",
    "    try:\n",
    "        print(f\"{source[:20]:<20} {num_articles_truth[source]:<10} {dataset.num_articles[source]:<10} {num_articles_truth[source] - dataset.num_articles[source]:<10}\")\n",
    "    except TypeError:\n",
    "        print(f\"{source[:20]:<20} {num_articles_truth[source]:<10} {dataset.num_articles[source]:<10} {'UNKNOWN':<10}\")\n",
    "\n",
    "# Compare true and empirical word counts and character counts\n",
    "print(f\"\\n{'':<20} {'Truth':<10} {'Empirical':<10} {'Difference':<10}\")\n",
    "print(f\"{'Word Count':<20} {word_count_truth:<10} {dataset.total_word_count:<10} {word_count_truth - dataset.total_word_count:<10}\")\n",
    "print(f\"{'Character Count':<20} {char_count_truth:<10} {dataset.total_char_count:<10} {char_count_truth - dataset.total_char_count:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function that takes in a single string and outputs a single d-dimensional vector\n",
    "def get_embedding(text):\n",
    "  # Use the embeddings OpenAI API endpoint to get an embedding for the text\n",
    "  result = openai.Embedding.create(model=EMBEDDING_MODEL, input=text)\n",
    "  # Convert the response to a numpy array and return it\n",
    "  return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Define a function that takes in a list of strings and outputs a numpy matrix of embeddings\n",
    "def get_embeddings(texts):\n",
    "  embeddings = []\n",
    "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(get_embedding, text) for text in texts]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "      embeddings.append(future.result())\n",
    "  return np.vstack(embeddings)\n",
    "\n",
    "def get_embeddings_not_parallel(texts):\n",
    "    embeddings = np.array([get_embedding(text) for text in texts])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Regular method\u001b[39;00m\n\u001b[0;32m      5\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m embeddings_1 \u001b[39m=\u001b[39m get_embeddings_not_parallel(texts)\n\u001b[0;32m      7\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRegular method: \u001b[39m\u001b[39m{\u001b[39;00mend \u001b[39m-\u001b[39m start\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mget_embeddings_not_parallel\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embeddings_not_parallel\u001b[39m(texts):\n\u001b[1;32m---> 18\u001b[0m     embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_embedding(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts])\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embeddings_not_parallel\u001b[39m(texts):\n\u001b[1;32m---> 18\u001b[0m     embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_embedding(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts])\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(text):\n\u001b[0;32m      3\u001b[0m   \u001b[39m# Use the embeddings OpenAI API endpoint to get an embedding for the text\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   result \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49mEMBEDDING_MODEL, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtext)\n\u001b[0;32m      5\u001b[0m   \u001b[39m# Convert the response to a numpy array and return it\u001b[39;00m\n\u001b[0;32m      6\u001b[0m   \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         ),\n\u001b[0;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists."
     ]
    }
   ],
   "source": [
    "# Define a list of texts to be embedded\n",
    "texts = [\"Hello world!\"] * 100\n",
    "\n",
    "# Regular method\n",
    "start = time.time()\n",
    "embeddings_1 = get_embeddings_not_parallel(texts)\n",
    "end = time.time()\n",
    "print(f\"Regular method: {end - start}\")\n",
    "\n",
    "# Parallel method\n",
    "start = time.time()\n",
    "embeddings_2 = get_embeddings(texts)\n",
    "end = time.time()\n",
    "print(f\"Parallel method: {end - start}\")\n",
    "\n",
    "print(embeddings_1.shape)\n",
    "print(embeddings_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for len(string): 0.0008568000048398972\n",
      "Average time for len(tiktoken): 15.112917799997376\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from timeit import timeit\n",
    "import string\n",
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "#we want to compare the speed of len(string) vs len(encoding.encode(string))\n",
    "#we will use a random string of length 1000 to do this\n",
    "\n",
    "def len_string(string):\n",
    "    return len(string)\n",
    "\n",
    "def len_tiktoken(string):\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "random_str = ''.join(random.choice(string.ascii_letters) for i in range(1000))\n",
    "\n",
    "#we will run each function 1000 times and compare the average time\n",
    "print(\"Average time for len(string):\", timeit(lambda: len_string(random_str), number=3000))\n",
    "print(\"Average time for len(tiktoken):\", timeit(lambda: len_tiktoken(random_str), number=3000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
