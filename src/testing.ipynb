{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```source: this key separates the various keys found in the table in Sources. Here's the set of sources with their corresponding value name:```\n",
    "```\n",
    "'https://aipulse.org'\n",
    "'ebook'\n",
    "'https://qualiacomputing.com'\n",
    "'alignment forum'\n",
    "'lesswrong'\n",
    "'manual'\n",
    "'arxiv'\n",
    "'https://deepmindsafetyresearch.medium.com/'\n",
    "'waitbutwhy.com'\n",
    "'GitHub'\n",
    "'https://aiimpacts.org'\n",
    "'arbital.com'\n",
    "'carado.moe'\n",
    "'nonarxiv_papers'\n",
    "'https://vkrakovna.wordpress.com'\n",
    "'https://jsteinhardt.wordpress.com'\n",
    "'audio-transcripts'\n",
    "'https://intelligence.org'\n",
    "'youtube'\n",
    "'reports'\n",
    "'https://aisafety.camp'\n",
    "'curriculum'\n",
    "'https://www.yudkowsky.net'\n",
    "'distill'\n",
    "```\n",
    "\n",
    "```...and this is how the arxiv papers look like:```\n",
    "\n",
    "```\n",
    "{\n",
    "    \"source\": \"arxiv\", # where the dataset comes from\n",
    "    \"source_type\": \"latex\", # the type of file the data was original in\n",
    "    \"converted_with\": \"pandoc\", # which tool we used to convert the data in .md format\n",
    "    \"paper_version\": paper_id,\n",
    "    \"title\": title,\n",
    "    \"authors\": [str(x) for x in authors], # list of authors\n",
    "    \"date_published\": date_published,\n",
    "    \"data_last_modified\": data_last_modified,\n",
    "    \"url\": url,\n",
    "    \"abstract\": abstract,\n",
    "    \"author_comment\": author_comment,\n",
    "    \"journal_ref\": journal_ref,\n",
    "    \"doi\": doi,\n",
    "    \"primary_category\": primary_category,\n",
    "    \"categories\": categories,\n",
    "    \"citation_level\": citation_level, # (0 = curated alignment papers, 1 = citation of curated papers, 2 = citation of citation, etc.)\n",
    "    \"alignment_text\": is_alignment_text, # 'pos' is maunally labeled as an alignment paper, 'unlabeled' if unlabeled\n",
    "    \"confidence_score\": confidence_scores, # this is a confidence score obtained by using the SPECTER model to classify papers to add to the dataset\n",
    "    \"main_tex_filename\": \"main.tex\", # the main latex file needed to convert the paper\n",
    "    \"text\": \"lots of text\", # this is where you will grab the text contents of each entry in the dataset (in .md format)\n",
    "    \"bibliography_bbl\": \"string of bbl\",\n",
    "    \"bibliography_bib\": \"string of bib\", # more common to have bib than bbl\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "\n",
    "import config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_EMBEDDINGS = 1536\n",
    "PATH_TO_DATA = r\"C:\\Users\\Henri\\Documents\\GitHub\\AlignmentSearch\\data\\alignment_texts.jsonl\"\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "MAX_LEN_PROMPT = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://stackoverflow.com/a/31505798/16185542\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"?!\", \"?\")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    if sentences == []:\n",
    "        sentences = [text.strip()]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_article(text: str) -> List[str]:\n",
    "    # Receives one text (str) and returns a list of sections (List[str]), each section being a few appended paragraphs that do not exceed 1000 words.\n",
    "    # This is done to avoid the 8000 token limit of OpenAI embeddings.\n",
    "    sections = []\n",
    "    section = \"\"\n",
    "    paragraphs = text.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph == \"\": continue\n",
    "        if len(section.split()) + len(paragraph.split()) > 1000 or len(section) + len(paragraph) > 7000:\n",
    "            sections.append(section)\n",
    "            section = \"\"\n",
    "        section += f\"{paragraph}\\n\"\n",
    "    sections.append(section)\n",
    "    return sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,\n",
    "            path: str,  # Path to the dataset .jsonl file.\n",
    "            sources: List[str] = None,  # List of sources to include. If None, include all sources.\n",
    "            max_paragraph_length: Tuple[int, int] = None  # (max number of words in a paragraph, max number of characters in a paragraph)\n",
    "        ):\n",
    "\n",
    "        self.path = path\n",
    "        self.sources = sources\n",
    "        self.max_paragraph_length = max_paragraph_length\n",
    "            \n",
    "        self.data: List[Tuple[str, str, str]] = []  # List of tuples, each containing the title of an article, its URL, and text. E.g.: [('title', 'url', 'text'), ...]\n",
    "        self.embed_split: List[str] = []  # List of strings, each being a few paragraphs from a single article (not exceeding 1000 words).\n",
    "        \n",
    "        self.num_articles: Dict[str, int] = {}  # Number of articles per source. E.g.: {'source1': 10, 'source2': 20, 'total': 30}\n",
    "        if sources is None:\n",
    "            self.num_articles['total'] = 0\n",
    "        else:\n",
    "            for source in sources: \n",
    "                self.num_articles[source] = 0\n",
    "            self.num_articles['total'] = 0\n",
    "        \n",
    "        self.total_char_count = 0\n",
    "        self.total_word_count = 0\n",
    "        self.total_sentence_count = 0\n",
    "        self.total_paragraph_count = 0\n",
    "        \n",
    "    def get_alignment_texts(self):\n",
    "        with jsonlines.open(self.path, \"r\") as reader:\n",
    "            for entry in reader:\n",
    "                try:\n",
    "                    if self.sources is None:\n",
    "                        if entry['source'] not in self.num_articles:\n",
    "                            self.num_articles[entry['source']] = 1\n",
    "                        else:\n",
    "                            self.num_articles[entry['source']] += 1\n",
    "                        self.num_articles['total'] += 1\n",
    "                    else:\n",
    "                        if entry['source'] in self.sources:\n",
    "                            self.num_articles[entry['source']] += 1\n",
    "                            self.num_articles['total'] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    # BIG PROBLEM: Very often, the post will have no URL, so this will fail.\n",
    "                    self.data.append((entry['title'], entry['url'], entry['text']))\n",
    "                    paragraphs = split_article(entry['text'])\n",
    "                    self.embed_split.extend(paragraphs)\n",
    "                    \n",
    "                    self.total_char_count += len(entry['text'])\n",
    "                    self.total_word_count += len(entry['text'].split())\n",
    "                    self.total_sentence_count += len(split_into_sentences(entry['text']))\n",
    "                    self.total_paragraph_count += len(paragraphs)\n",
    "                except KeyError: # TO BE CHANGED\n",
    "                    pass\n",
    "    \n",
    "    def get_embedding(text: str) -> np.ndarray:\n",
    "        result = openai.Embedding.create(model=EMBEDDING_MODEL, input=text)\n",
    "        return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.embeddings = np.array([self.get_embedding(text) for text in self.embed_split])\n",
    "    \n",
    "    def save_embeddings(self, path: str):\n",
    "        np.save(path, self.embeddings)\n",
    "        \n",
    "    def load_embeddings(self, path: str):\n",
    "        self.embeddings = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAndAnswer:\n",
    "    def __init__(self,\n",
    "            dataset: Dataset,  # Dataset object containing the data.\n",
    "        ):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        result = openai.Embedding.create(model=EMBEDDING_MODEL, input=text)\n",
    "        return result[\"data\"][0][\"embedding\"]\n",
    "    \n",
    "    def get_top_k(self, query: str, k: int=10) -> List[Tuple[str, str, str]]:\n",
    "        # Receives a query (str) and returns the top k articles (List[Tuple[str, str, str]]) that are most similar to the query.\n",
    "        # Each tuple contains the title of an article, its URL, and text.\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        similarities = np.dot(self.dataset.embeddings, query_embedding)\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_k = [self.dataset.data[i] for i in top_k_indices]\n",
    "        return top_k\n",
    "    \n",
    "    def construct_prompt(self, question: str, texts: List[Tuple[str, str, str]]) -> str:\n",
    "        # Receives a question (str) and a list of articles (List[Tuple[str, str, str]]) and returns a prompt (str) to be used for text generation.\n",
    "        context = \"\\n\".join(texts)[:MAX_LEN_PROMPT]\n",
    "        header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "        return header + \"\".join(context) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "    \n",
    "    def answer_question(self, question: str, texts: List[Tuple[str, str, str]]) -> str:\n",
    "        # Receives a question (str) and a list of articles (List[Tuple[str, str, str]]) and returns an answer (str) to the question.\n",
    "        prompt = self.construct_prompt(question, texts)\n",
    "        COMPLETIONS_API_PARAMS = {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 500,\n",
    "            \"model\": COMPLETIONS_MODEL,\n",
    "        }\n",
    "        answer = openai.Completion.create(prompt=prompt, **COMPLETIONS_API_PARAMS)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "        return answer\n",
    "    \n",
    "    def search_and_answer(self, question: str, k: int=10) -> str:\n",
    "        # Receives a question (str) and returns an answer (str) to the question.\n",
    "        top_k = self.get_top_k(question, k)\n",
    "        answer = self.answer_question(question, top_k)\n",
    "        return answer\n",
    "    \n",
    "    def summarize(self, article: str) -> str:\n",
    "        COMPLETIONS_API_PARAMS = {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 300,\n",
    "            \"model\": COMPLETIONS_MODEL,\n",
    "        }\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(path=PATH_TO_DATA, sources=['https://www.yudkowsky.net'])\n",
    "dataset.get_alignment_texts()\n",
    "# dataset.get_embeddings()\n",
    "# dataset.save_embeddings(EMBEDDINGS_PATH)\n",
    "# # dataset.load_embeddings(EMBEDDINGS_PATH)\n",
    "\n",
    "# search_and_answer = SearchAndAnswer(dataset)\n",
    "\n",
    "# while True:\n",
    "#     question = input(\"Enter a question: \")\n",
    "#     if question == \"quit\":\n",
    "#         break\n",
    "#     top_k = search_and_answer.get_top_k(question)\n",
    "#     answer = search_and_answer.answer_question(question, top_k)\n",
    "#     print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://www.yudkowsky.net': 23, 'total': 23}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source               Truth      Empirical  Difference\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# Print table. First row has Truth and Empirical findings.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mSource\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m<20\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mTruth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mEmpirical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mDifference\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfor\u001b[39;00m source \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mnum_articles:\n\u001b[0;32m     52\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msource[:\u001b[39m20\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m<20\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnum_articles_truth[source]\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mnum_articles[source]\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnum_articles_truth[source] \u001b[39m-\u001b[39m dataset\u001b[39m.\u001b[39mnum_articles[source]\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# self.path = path\n",
    "# self.sources = sources\n",
    "# self.max_data_length = max_data_length\n",
    "# self.len_embeddings = len_embeddings\n",
    "\n",
    "\n",
    "# self.data: List[Tuple[str, str, str]] = []  # List of tuples, each containing the title of an article, its URL, and text. E.g.: [('title', 'url', 'text'), ...]\n",
    "# self.embed_split: List[str] = []  # List of strings, each being a few paragraphs from a single article (not exceeding 1000 words).\n",
    "\n",
    "# self.num_articles: Dict[str, int] = {}  # Dict of number of articles from each source, with total number of articles. Initialize num_articles to 0 for each source.\n",
    "# for source in sources: self.num_articles[source] = 0\n",
    "# self.num_articles['total'] = 0\n",
    "\n",
    "# self.total_char_count = 0\n",
    "# self.total_word_count = 0\n",
    "# self.total_sentence_count = 0\n",
    "# self.total_paragraph_count = 0\n",
    "\n",
    "num_articles_truth = {\n",
    "    'https://aipulse.org': 23,\n",
    "    'ebook': 23,\n",
    "    'https://qualiacomputing.com': 278,\n",
    "    'alignment forum': 2138,\n",
    "    'lesswrong': 28252 + 227,\n",
    "    'manual': \"?\",\n",
    "    'arxiv': 707 + 1679 + 1000 + 4621,\n",
    "    'https://deepmindsafetyresearch.medium.com/': 10,\n",
    "    'waitbutwhy.com': 2,\n",
    "    'GitHub': \"?\",\n",
    "    'https://aiimpacts.org': 227,\n",
    "    'arbital.com': 223,\n",
    "    'carado.moe': 59,\n",
    "    'nonarxiv_papers': \"?\",\n",
    "    'https://vkrakovna.wordpress.com': 43,\n",
    "    'https://jsteinhardt.wordpress.com': 39,\n",
    "    'audio-transcripts': 25 + 12,\n",
    "    'https://intelligence.org': 479,\n",
    "    'youtube': 457,\n",
    "    'reports': \"?\",\n",
    "    'https://aisafety.camp': 8,\n",
    "    'curriculum': \"?\",\n",
    "    'https://www.yudkowsky.net': 23,\n",
    "    'distill': 49,\n",
    "    'total': 2138+28252+707+1679+1000+4621+23+227+23+8+59+111+10+17+7+479+39+278+43+2+23+420+323+49+457+25+12+223+227+132    \n",
    "}\n",
    "word_count_truth = 53_550_146\n",
    "char_count_truth = 351_767_163\n",
    "\n",
    "# Print table. First row has Truth and Empirical findings.\n",
    "print(f\"{'Source':<20} {'Truth':<10} {'Empirical':<10} {'Difference':<10}\")\n",
    "for source in dataset.num_articles:\n",
    "    try:\n",
    "        print(f\"{source[:20]:<20} {num_articles_truth[source]:<10} {dataset.num_articles[source]:<10} {num_articles_truth[source] - dataset.num_articles[source]:<10}\")\n",
    "    except TypeError:\n",
    "        print(f\"{source[:20]:<20} {num_articles_truth[source]:<10} {dataset.num_articles[source]:<10} {'UNKNOWN':<10}\")\n",
    "\n",
    "# Compare true and empirical word counts and character counts\n",
    "print(f\"\\n{'':<20} {'Truth':<10} {'Empirical':<10} {'Difference':<10}\")\n",
    "print(f\"{'Word Count':<20} {word_count_truth:<10} {dataset.total_word_count:<10} {word_count_truth - dataset.total_word_count:<10}\")\n",
    "print(f\"{'Character Count':<20} {char_count_truth:<10} {dataset.total_char_count:<10} {char_count_truth - dataset.total_char_count:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348457759 characters\n",
      "~104537328 tokens\n",
      "~69691552 words\n",
      "~348457 paragraphs\n",
      "~12912 embeddings using method 1\n",
      "~116152 embeddings using method 2\n",
      "~139383 pages\n",
      "~46 cost using method 1\n",
      "~627 cost using method 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_words = dataset.data_length / 5\n",
    "num_tokens = num_words * 1.5\n",
    "num_paragraphs = num_words // 200\n",
    "num_embeds_method_1 = num_tokens // 8096\n",
    "num_embeds_method_2 = num_words // 600\n",
    "cost_per_embed = 1/(3000*500/8096)\n",
    "cost_per_page = 1/3000\n",
    "num_pages = num_words // 500\n",
    "cost_1 = num_pages * cost_per_page\n",
    "cost_2 = num_embeds_method_2 * cost_per_embed\n",
    "\n",
    "print(f\"{dataset.data_length} characters\")\n",
    "print(f\"~{num_tokens:.0f} tokens\")\n",
    "print(f\"~{num_words:.0f} words\")\n",
    "print(f\"~{num_paragraphs:.0f} paragraphs\")\n",
    "print(f\"~{num_embeds_method_1:.0f} embeddings using method 1\")\n",
    "print(f\"~{num_embeds_method_2:.0f} embeddings using method 2\")\n",
    "print(f\"~{num_pages:.0f} pages\")\n",
    "print(f\"~{cost_1:.0f} cost using method 1\")\n",
    "print(f\"~{cost_2:.0f} cost using method 2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(PATH_TO_DATA, \"r\") as reader, open(\"aipulse.txt\", \"w\", encoding=\"utf-8\") as writer:\n",
    "    for entry in reader:\n",
    "        try:\n",
    "            if 'source' in entry and entry['source'] == 'https://aipulse.org':\n",
    "                if 'title' in entry:\n",
    "                    writer.write(f\"Title: {entry['title']}\\n\")\n",
    "                else:\n",
    "                    writer.write(f\"NO TITLE\\n\")\n",
    "                if 'text' in entry:\n",
    "                    writer.write(f\"Text: {entry['text']}\\n\")\n",
    "                else:\n",
    "                    writer.write(f\"NO TEXT\\n\")\n",
    "                if 'url' in entry:\n",
    "                    writer.write(f\"URL: {entry['url']}\\n\")\n",
    "                else:\n",
    "                    writer.write(f\"NO URL\\n\")\n",
    "                writer.write(\"\\n\\n\")\n",
    "            else:\n",
    "                continue\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
