{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```source: this key separates the various keys found in the table in Sources. Here's the set of sources with their corresponding value name:```\n",
    "```\n",
    "'https://aipulse.org'\n",
    "'ebook'\n",
    "'https://qualiacomputing.com'\n",
    "'alignment forum'\n",
    "'lesswrong'\n",
    "'manual'\n",
    "'arxiv'\n",
    "'https://deepmindsafetyresearch.medium.com/'\n",
    "'waitbutwhy.com'\n",
    "'GitHub'\n",
    "'https://aiimpacts.org'\n",
    "'arbital.com'\n",
    "'carado.moe'\n",
    "'nonarxiv_papers'\n",
    "'https://vkrakovna.wordpress.com'\n",
    "'https://jsteinhardt.wordpress.com'\n",
    "'audio-transcripts'\n",
    "'https://intelligence.org'\n",
    "'youtube'\n",
    "'reports'\n",
    "'https://aisafety.camp'\n",
    "'curriculum'\n",
    "'https://www.yudkowsky.net'\n",
    "'distill'\n",
    "```\n",
    "\n",
    "```...and this is how the arxiv papers look like:```\n",
    "\n",
    "```\n",
    "{\n",
    "    \"source\": \"arxiv\", # where the dataset comes from\n",
    "    \"source_type\": \"latex\", # the type of file the data was original in\n",
    "    \"converted_with\": \"pandoc\", # which tool we used to convert the data in .md format\n",
    "    \"paper_version\": paper_id,\n",
    "    \"title\": title,\n",
    "    \"authors\": [str(x) for x in authors], # list of authors\n",
    "    \"date_published\": date_published,\n",
    "    \"data_last_modified\": data_last_modified,\n",
    "    \"url\": url,\n",
    "    \"abstract\": abstract,\n",
    "    \"author_comment\": author_comment,\n",
    "    \"journal_ref\": journal_ref,\n",
    "    \"doi\": doi,\n",
    "    \"primary_category\": primary_category,\n",
    "    \"categories\": categories,\n",
    "    \"citation_level\": citation_level, # (0 = curated alignment papers, 1 = citation of curated papers, 2 = citation of citation, etc.)\n",
    "    \"alignment_text\": is_alignment_text, # 'pos' is maunally labeled as an alignment paper, 'unlabeled' if unlabeled\n",
    "    \"confidence_score\": confidence_scores, # this is a confidence score obtained by using the SPECTER model to classify papers to add to the dataset\n",
    "    \"main_tex_filename\": \"main.tex\", # the main latex file needed to convert the paper\n",
    "    \"text\": \"lots of text\", # this is where you will grab the text contents of each entry in the dataset (in .md format)\n",
    "    \"bibliography_bbl\": \"string of bbl\",\n",
    "    \"bibliography_bib\": \"string of bib\", # more common to have bib than bbl\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_EMBEDDINGS = 1536\n",
    "PATH_TO_DATA = r\"C:\\Users\\Henri\\Documents\\GitHub\\AlignmentSearch\\data\\alignment_texts.jsonl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://stackoverflow.com/a/31505798/16185542\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"?!\", \"?\")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    if sentences == []:\n",
    "        sentences = [text.strip()]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_article(text: str) -> List[str]:\n",
    "    # Receives one text (str) and returns a list of sections (List[str]), each section being a few appended paragraphs that do not exceed 1000 words.\n",
    "    # This is done to avoid the 8000 token limit of OpenAI embeddings.\n",
    "    sections = []\n",
    "    section = \"\"\n",
    "    paragraphs = text.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph == \"\": continue\n",
    "        if len(section.split()) + len(paragraph.split()) > 1000 or len(section) + len(paragraph) > 7000:\n",
    "            sections.append(section)\n",
    "            section = \"\"\n",
    "        section += f\"{paragraph}\\n\"\n",
    "    sections.append(section)\n",
    "    return sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,\n",
    "            path: str,  # Path to the dataset .jsonl file.\n",
    "            sources: List[str] = None,  # List of sources to include. If None, include all sources.\n",
    "            max_paragraph_length: Tuple[int, int] = None,  # (max number of words in a paragraph, max number of characters in a paragraph)\n",
    "            load_data: bool = False,  # Whether to load the data from the .jsonl file at initialization.\n",
    "            load_embeddings: bool = False,  # Whether to load the embeddings from the .npy file at initialization. (only available if load_data is True)\n",
    "        ):\n",
    "        self.path = path\n",
    "        self.sources = sources\n",
    "        self.max_paragraph_length = max_paragraph_length\n",
    "            \n",
    "        self.data: List[Tuple[str, str, str]] = []  # List of tuples, each containing the title of an article, its URL, and text. E.g.: [('title', 'url', 'text'), ...]\n",
    "        self.embed_split: List[str] = []  # List of strings, each being a few paragraphs from a single article (not exceeding 1000 words).\n",
    "        \n",
    "        self.num_articles: Dict[str, int] = {}  # Number of articles per source. E.g.: {'source1': 10, 'source2': 20, 'total': 30}\n",
    "        if sources is None:\n",
    "            self.num_articles['total'] = 0\n",
    "        else:\n",
    "            for source in sources: \n",
    "                self.num_articles[source] = 0\n",
    "            self.num_articles['total'] = 0\n",
    "        \n",
    "        self.total_char_count = 0\n",
    "        self.total_word_count = 0\n",
    "        self.total_sentence_count = 0\n",
    "        self.total_paragraph_count = 0\n",
    "        \n",
    "        if load_data:\n",
    "            self.load()\n",
    "            if load_embeddings:\n",
    "                self.load_embeddings()\n",
    "    \n",
    "    def load(self):\n",
    "        with jsonlines.open(self.path, \"r\") as reader:\n",
    "            for entry in reader:\n",
    "                try:\n",
    "                    if self.sources is None:\n",
    "                        if entry['source'] not in self.num_articles:\n",
    "                            self.num_articles[entry['source']] = 1\n",
    "                        else:\n",
    "                            self.num_articles[entry['source']] += 1\n",
    "                        self.num_articles['total'] += 1\n",
    "                    else:\n",
    "                        if entry['source'] in self.sources:\n",
    "                            self.num_articles[entry['source']] += 1\n",
    "                            self.num_articles['total'] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    self.data.append((entry['title'], entry['url'], entry['text']))\n",
    "                    paragraphs = split_article(entry['text'])\n",
    "                    self.embed_split.extend(paragraphs)\n",
    "                    \n",
    "                    self.total_char_count += len(entry['text'])\n",
    "                    self.total_word_count += len(entry['text'].split())\n",
    "                    self.total_sentence_count += len(split_into_sentences(entry['text']))\n",
    "                    self.total_paragraph_count += len(paragraphs)\n",
    "                except KeyError: # TO BE CHANGED\n",
    "                    pass\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worthwhile_sources = [\n",
    "#     'https://aipulse.org',\n",
    "#     'ebook',\n",
    "#     'https://qualiacomputing.com',\n",
    "#     'alignment forum',\n",
    "#     'lesswrong',\n",
    "#     'manual',\n",
    "#     'arxiv',\n",
    "#     'https://deepmindsafetyresearch.medium.com/',\n",
    "#     'waitbutwhy.com',\n",
    "#     'GitHub',\n",
    "#     'https://aiimpacts.org',\n",
    "#     'arbital.com',\n",
    "#     'carado.moe',\n",
    "#     'nonarxiv_papers',\n",
    "#     'https://vkrakovna.wordpress.com',\n",
    "#     'https://jsteinhardt.wordpress.com',\n",
    "#     'audio-transcripts',\n",
    "#     'https://intelligence.org',\n",
    "#     'youtube',\n",
    "#     'reports',\n",
    "#     'https://aisafety.camp',\n",
    "#     'curriculum',\n",
    "#     'https://www.yudkowsky.net',\n",
    "#     'distill'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(path=PATH_TO_DATA, sources=None, load_data=True, load_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74845\n"
     ]
    }
   ],
   "source": [
    "len_embeds = []\n",
    "for embed in dataset.embed_split:\n",
    "    len_embeds.append(len(embed.split()))\n",
    "# Find argmax\n",
    "print(np.argmax(len_embeds)) # RESPONSE: 12502\n",
    "# print(max(len_embeds)) # RESPONSE: 12502; This is bad news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source               Truth      Empirical \n",
      "total                41614      39713     \n",
      "https://aipulse.org  23         23        \n",
      "ebook                23         22        \n",
      "https://qualiacomput 278        278       \n",
      "alignment forum      2138       2138      \n",
      "lesswrong            28479      28259     \n",
      "manual               132        1         \n",
      "arxiv                8007       7012      \n",
      "https://deepmindsafe 10         10        \n",
      "waitbutwhy.com       2          2         \n",
      "GitHub               0          1         \n",
      "https://aiimpacts.or 227        227       \n",
      "arbital.com          223        223       \n",
      "carado.moe           59         59        \n",
      "nonarxiv_papers      323        244       \n",
      "https://vkrakovna.wo 43         43        \n",
      "https://jsteinhardt. 39         39        \n",
      "audio-transcripts    25         37        \n",
      "https://intelligence 479        479       \n",
      "youtube              457        457       \n",
      "reports              323        78        \n",
      "https://aisafety.cam 8          8         \n",
      "curriculum           0          1         \n",
      "https://www.yudkowsk 23         23        \n",
      "distill              49         49        \n",
      "\n",
      "                     Truth      Empirical \n",
      "Word Count           53550146   44501538  \n",
      "Character Count      351767163  294346152 \n"
     ]
    }
   ],
   "source": [
    "# self.path = path\n",
    "# self.sources = sources\n",
    "# self.max_data_length = max_data_length\n",
    "# self.len_embeddings = len_embeddings\n",
    "\n",
    "\n",
    "# self.data: List[Tuple[str, str, str]] = []  # List of tuples, each containing the title of an article, its URL, and text. E.g.: [('title', 'url', 'text'), ...]\n",
    "# self.embed_split: List[str] = []  # List of strings, each being a few paragraphs from a single article (not exceeding 1000 words).\n",
    "\n",
    "# self.num_articles: Dict[str, int] = {}  # Dict of number of articles from each source, with total number of articles. Initialize num_articles to 0 for each source.\n",
    "# for source in sources: self.num_articles[source] = 0\n",
    "# self.num_articles['total'] = 0\n",
    "\n",
    "# self.total_char_count = 0\n",
    "# self.total_word_count = 0\n",
    "# self.total_sentence_count = 0\n",
    "# self.total_paragraph_count = 0\n",
    "\n",
    "num_articles_truth = {\n",
    "    'https://aipulse.org': 23,\n",
    "    'ebook': 23,\n",
    "    'https://qualiacomputing.com': 278,\n",
    "    'alignment forum': 2138,\n",
    "    'lesswrong': 28252 + 227,\n",
    "    'manual': 132, # Stampy.ai?\n",
    "    'arxiv': 707 + 1679 + 1000 + 4621,\n",
    "    'https://deepmindsafetyresearch.medium.com/': 10,\n",
    "    'waitbutwhy.com': 2,\n",
    "    'GitHub': 0, # Huh?\n",
    "    'https://aiimpacts.org': 227,\n",
    "    'arbital.com': 223,\n",
    "    'carado.moe': 59,\n",
    "    'nonarxiv_papers': 323,\n",
    "    'https://vkrakovna.wordpress.com': 43,\n",
    "    'https://jsteinhardt.wordpress.com': 39,\n",
    "    'audio-transcripts': 25 + 12,\n",
    "    'https://intelligence.org': 479,\n",
    "    'youtube': 457,\n",
    "    'reports': 323,\n",
    "    'https://aisafety.camp': 8,\n",
    "    'curriculum': 0, # Huh?\n",
    "    'https://www.yudkowsky.net': 23,\n",
    "    'distill': 49,\n",
    "    'total': 2138+28252+707+1679+1000+4621+23+227+23+8+59+111+10+17+7+479+39+278+43+2+23+420+323+49+457+25+12+223+227+132    \n",
    "}\n",
    "word_count_truth = 53_550_146\n",
    "char_count_truth = 351_767_163\n",
    "\n",
    "# Print table. First row has Truth and Empirical findings.\n",
    "print(f\"{'Source':<20} {'Truth':<10} {'Empirical':<10}\")\n",
    "for source in dataset.num_articles:\n",
    "    print(f\"{source[:20]:<20} {num_articles_truth[source]:<10} {dataset.num_articles[source]:<10}\")\n",
    "\n",
    "# Compare true and empirical word counts and character counts\n",
    "print(f\"\\n{'':<20} {'Truth':<10} {'Empirical':<10}\")\n",
    "print(f\"{'Word Count':<20} {word_count_truth:<10} {dataset.total_word_count:<10}\")\n",
    "print(f\"{'Character Count':<20} {char_count_truth:<10} {dataset.total_char_count:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348457759 characters\n",
      "~104537328 tokens\n",
      "~69691552 words\n",
      "~348457 paragraphs\n",
      "~12912 embeddings using method 1\n",
      "~116152 embeddings using method 2\n",
      "~139383 pages\n",
      "~46 cost using method 1\n",
      "~627 cost using method 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_words = dataset.data_length / 5\n",
    "num_tokens = num_words * 1.5\n",
    "num_paragraphs = num_words // 200\n",
    "num_embeds_method_1 = num_tokens // 8096\n",
    "num_embeds_method_2 = num_words // 600\n",
    "cost_per_embed = 1/(3000*500/8096)\n",
    "cost_per_page = 1/3000\n",
    "num_pages = num_words // 500\n",
    "cost_1 = num_pages * cost_per_page\n",
    "cost_2 = num_embeds_method_2 * cost_per_embed\n",
    "\n",
    "print(f\"{dataset.data_length} characters\")\n",
    "print(f\"~{num_tokens:.0f} tokens\")\n",
    "print(f\"~{num_words:.0f} words\")\n",
    "print(f\"~{num_paragraphs:.0f} paragraphs\")\n",
    "print(f\"~{num_embeds_method_1:.0f} embeddings using method 1\")\n",
    "print(f\"~{num_embeds_method_2:.0f} embeddings using method 2\")\n",
    "print(f\"~{num_pages:.0f} pages\")\n",
    "print(f\"~{cost_1:.0f} cost using method 1\")\n",
    "print(f\"~{cost_2:.0f} cost using method 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187.5"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000*500/8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1$ per 2000 embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 1D array of 1536 (very small) random numbers which add up to 1, and a 2D array of shape (1536, 100000), and for each row the sum of the elements is 1.\n",
    "arr1 = np.random.rand(1536)\n",
    "arr1 /= arr1.sum()\n",
    "\n",
    "arr2 = np.random.rand(1536, 100000)\n",
    "arr2 /= arr2.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.8 ms ± 2.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Now, calculate the amount of time it takes to take the dot product of the two arrays.\n",
    "%timeit arr1.dot(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3 = arr1.dot(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# top k argmax\u001b[39;00m\n\u001b[0;32m      2\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> 3\u001b[0m top_k \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(arr3, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[:k]\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# top k argmax\n",
    "k = 10\n",
    "top_k = np.argmax(arr3, axis=0)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "openai.api_key = config.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_unit_vectors(d, n):\n",
    "    vec = np.random.randn(d, n)\n",
    "    norm = np.linalg.norm(vec, axis=0)\n",
    "    return vec / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1536\n",
    "n = 1000\n",
    "rand_mat = random_unit_vectors(d, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 1000)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return np.array(result[\"data\"][0][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_embedding(\"JSON stands for JavaScript Object Notation. It means that a script (executable) file which is made of text in a programming language, is used to store and transfer the data. Python supports JSON through a built-in package called json. To use this feature, we import the json package in Python script. The text in JSON is done through quoted-string which contains value in key-value mapping within { }. It is similar to the dictionary in Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 100000)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02933221, 0.02961996, 0.02955815, ..., 0.02957929, 0.02946535,\n",
       "       0.02934775])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the distance of a to origin\n",
    "np.linalg.norm(arr2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 ms ± 2.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a.dot(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00081222])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %timeit np.argmax(a.dot(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.dot(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97841,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[b<0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
